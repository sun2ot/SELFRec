{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ds_util import head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_dataset_path = '/nvme0n1p2/yelp_dataset/'\n",
    "yelp_photo_path = '/nvme0n1p2/yelp_photos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交互数据\n",
    "\n",
    "## Step1：从 review 数据集中提取所有用户，并统计每个用户的评论数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "# 加载 user_id 为集合\n",
    "user_ids = set()\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_user.json', 'r', encoding='utf-8') as f_users:\n",
    "    for line in f_users:\n",
    "        user_data = json.loads(line)\n",
    "        user_ids.add(user_data['user_id'])\n",
    "\n",
    "# 统计每个 user_id 的实际 reviews 数量\n",
    "user_review_count = {}\n",
    "\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        user_id = review_data['user_id']\n",
    "        if user_id in user_ids:\n",
    "            if user_id in user_review_count:\n",
    "                user_review_count[user_id] += 1\n",
    "            else:\n",
    "                user_review_count[user_id] = 1\n",
    "\n",
    "# 保存为本地文件\n",
    "with open('yelp_out/user_review_count.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for user_id, count in user_review_count.items():\n",
    "        output_file.write(f\"{user_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_-eMZ6K5RLWhZyISBhwA 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/user_review_count.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2：筛选出交互次数（评论数）大于等于 10 次的用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "items = {}\n",
    "with open('yelp_out/user_review_count.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # count_data -> [user_id, review_count]\n",
    "        count_data = line.strip().split(' ')\n",
    "        if int(count_data[1]) >= 10:\n",
    "            items[count_data[0]] = count_data[1]\n",
    "        if int(count_data[1]) < 1:\n",
    "            print(f'user {count_data[0]} has 0 review!')\n",
    "\n",
    "# 保存为本地文件\n",
    "with open('yelp_out/core_users.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for user_id, count in items.items():\n",
    "        output_file.write(f\"{user_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_-eMZ6K5RLWhZyISBhwA 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/core_users.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3：从 review 数据集中提取所有项目商家，并统计每个商家的评论数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "# 加载 business_id 为集合\n",
    "business_ids = set()\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_business.json', 'r', encoding='utf-8') as f_item:\n",
    "    for line in f_item:\n",
    "        item_data = json.loads(line)\n",
    "        business_ids.add(item_data['business_id'])\n",
    "\n",
    "# 统计每个 business_id 的实际 reviews 数量\n",
    "item_review_count = {}\n",
    "\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        business_id = review_data['business_id']\n",
    "        if business_id in business_ids:\n",
    "            if business_id in item_review_count:\n",
    "                item_review_count[business_id] += 1\n",
    "            else:\n",
    "                item_review_count[business_id] = 1\n",
    "\n",
    "# 保存为本地文件\n",
    "with open('yelp_out/item_review_count.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for business_id, count in item_review_count.items():\n",
    "        output_file.write(f\"{business_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQfwVwDr-v0ZS3_CbbE5Xw 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/item_review_count.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4：筛选出交互次数大于等于 10 次且存在 photos 数据的商家\n",
    "\n",
    "理论上说，10-core settings 只需要交互次数限制就可以了。但本文由于需要图片模态数据，而实际处理过程中发现如果不添加“有图片”的限制条件，生成的数据集中，将有约 67% 的 business 没有对应的 photos 数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "items = {}\n",
    "business_with_photos = set()\n",
    "\n",
    "try:\n",
    "    with open(yelp_photo_path + 'photos.json', 'r') as photo_list:\n",
    "        for line in photo_list:\n",
    "            data = json.loads(line)\n",
    "            business_id = data['business_id']\n",
    "            business_with_photos.add(business_id)\n",
    "\n",
    "    with open('yelp_out/item_review_count.txt', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # count_data -> [business_id, review_count]\n",
    "            count_data = line.strip().split(' ')\n",
    "            if int(count_data[1]) >= 10 and count_data[0] in business_with_photos:\n",
    "                items[count_data[0]] = count_data[1]\n",
    "            if int(count_data[1]) < 1:\n",
    "                print(f'business {count_data[0]} has 0 review!')\n",
    "\n",
    "    # 保存为本地文件\n",
    "    with open('yelp_out/re_core_items.txt', 'w', encoding='utf-8') as output_file:\n",
    "        for business_id, count in items.items():\n",
    "            output_file.write(f\"{business_id} {count}\\n\")\n",
    "        \n",
    "        print(\"generate complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'errors: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQfwVwDr-v0ZS3_CbbE5Xw 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/core_items.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5：交叉过滤-从 review 数据集中提取出 10-core 交互记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，yelp 数据集自带换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        if '\\n' in line: \n",
    "            print('yes')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，由于存在一个用户对同一商家的多条评论记录，因此构造过滤 review 数据集时，需考虑以下几方面：\n",
    "1. 用户和项目都是核心用户/项目\n",
    "2. (user_id ,business_id) 不重复\n",
    "3. 针对重复的情况，取评分最高项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter reviews done\n"
     ]
    }
   ],
   "source": [
    "core_users = set()\n",
    "items = set()\n",
    "# 保证一个用户对一个项目只有一条交互记录\n",
    "user_item_pairs = {}\n",
    "\n",
    "with open('yelp_out/core_users.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        user_id = line.strip().split(' ')[0]\n",
    "        core_users.add(user_id)\n",
    "\n",
    "with open('yelp_out/re_core_items.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        business_id = line.strip().split(' ')[0]\n",
    "        items.add(business_id)\n",
    "\n",
    "filter_reviews = []  # -> [user_id, business_id, stars, review_id]\n",
    "index = 0\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        user_id = review_data['user_id']\n",
    "        business_id = review_data['business_id']\n",
    "\n",
    "        if user_id in core_users and business_id in items:\n",
    "            if (user_id, business_id) not in user_item_pairs:\n",
    "                # 用哈希表同时记录评分和索引，以便后续操作\n",
    "                user_item_pairs[(user_id, business_id)] = [review_data['stars'], index]\n",
    "                # filter_reviews.append([user_id, business_id, review_data['stars'], review_data['review_id']])\n",
    "                filter_reviews.append([user_id, business_id, review_data['stars']])\n",
    "                index += 1\n",
    "            else:\n",
    "                if review_data['stars'] > user_item_pairs[(user_id, business_id)][0]:\n",
    "                    pair_index = user_item_pairs[(user_id, business_id)][1]\n",
    "                    # 更新数据\n",
    "                    # filter_reviews[pair_index] = [user_id, business_id, review_data['stars'], review_data['review_id']]\n",
    "                    filter_reviews[pair_index] = [user_id, business_id, review_data['stars']]\n",
    "\n",
    "# 持久化过滤后的评论数据集\n",
    "with open('yelp_out/re_yelp_interactions.txt', 'w', encoding='utf-8') as out_file:\n",
    "    for record in filter_reviews:\n",
    "        out_file.write(' '.join(map(str, record)) + '\\n')\n",
    "    print('Filter reviews done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcjbaE6dDog4jkNY91ncLQ e4Vwtrqf-wpJfwesgvdgxQ 4.0\n",
      "\n",
      "smOvOajNG0lS4Pq7d8g4JQ RZtGWDLCAtuipwaZ-UfjmQ 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/re_yelp_interactions.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6：划分数据集\n",
    "\n",
    "参考 [SELFRec issue 54](https://github.com/Coder-Yu/SELFRec/issues/54)，领域内在得到最终结果时，似乎会把训练集和验证集合并，所以这里先按 8:2 划分训练集跟测试集，然后在将训练集划分为 7:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据重复行数: 0\n",
      "训练集大小: 1429167\n",
      "测试集大小: 612501\n",
      "训练数据重复行数: 0\n",
      "测试数据重复行数: 0\n",
      "训练集和测试集中的重复行:\n",
      "Empty DataFrame\n",
      "Columns: [0, 1, 2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('yelp_out/re_yelp_interactions.txt', delimiter=' ', header=None)\n",
    "\n",
    "# 检查重复行\n",
    "print(f\"原始数据重复行数: {data.duplicated().sum()}\")\n",
    "\n",
    "# 划分训练集和测试集，比例为8:2\n",
    "temp_data, test_data = train_test_split(data, test_size=0.2, random_state=114514)\n",
    "\n",
    "temp_data.to_csv('yelp_ds_re/train_data.txt', index=False, header=False, sep=' ')\n",
    "test_data.to_csv('yelp_ds_re/test_data.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "# 查看划分后的数据集大小\n",
    "print(f\"训练集大小: {len(temp_data)}\")\n",
    "print(f\"测试集大小: {len(test_data)}\")\n",
    "\n",
    "# 检查是否有重复项\n",
    "print(f\"训练数据重复行数: {temp_data.duplicated().sum()}\")\n",
    "print(f\"测试数据重复行数: {test_data.duplicated().sum()}\")\n",
    "duplicates_in_train = temp_data.merge(test_data, how='inner')\n",
    "print(f'训练集和测试集中的重复行:\\n{duplicates_in_train}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 根据生成的 interactions 数据，对第二列建立集合，即数据集中所有 business_id\n",
    "2. 将 business_id 代入 `photos/` 检索出所有 photo_id 并保存文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像记录数据展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"photo_id\": \"zsvj7vloL4L5jhYyPIuVwg\",\n",
      "  \"business_id\": \"Nk-SJhPlDBkAZvfsADtccA\",\n",
      "  \"caption\": \"Nice rock artwork everywhere and craploads of taps.\",\n",
      "  \"label\": \"inside\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "head(yelp_photo_path + 'photos.json', jsonf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 商家-图片映射记录\n",
    "\n",
    "从最终的**交互数据集**中，提取出商家-图片(一对多)的映射关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "photo_ids = set()\n",
    "items = set()\n",
    "item2photos = {}\n",
    "\n",
    "try:\n",
    "    with open('yelp_out/re_yelp_interactions.txt', 'r') as interact_file:\n",
    "        for line in interact_file:\n",
    "            items.add(line.split(' ')[1])\n",
    "            item2photos[line.split(' ')[1]] = []\n",
    "    \n",
    "    # item: photo_id1, photo_id2, ...\n",
    "    with open('yelp_out/item2photos.txt', 'w', encoding='utf-8') as out_file:\n",
    "        with open(yelp_photo_path + 'photos.json', 'r', encoding='utf-8') as photos:\n",
    "            for line in photos:\n",
    "                json_data = json.loads(line)\n",
    "                photo_id = json_data['photo_id']\n",
    "                business_id = json_data['business_id']\n",
    "\n",
    "                # 存在 photo_id 重复的情况\n",
    "                if photo_id not in photo_ids and business_id in items:\n",
    "                    item2photos[business_id].append(photo_id)\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        for item in item2photos:\n",
    "            out_file.write(item + ' ' + ' '.join(item2photos[item]) + '\\n')\n",
    "        print('generate complete')\n",
    "except Exception as e:\n",
    "    print(f'error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 business without any photo\n",
      "occupy 0.0%\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/item2photos.txt', 'r') as f:\n",
    "    no_photos_item = 0\n",
    "    total_item = 0\n",
    "\n",
    "    for line in f:\n",
    "        total_item += 1\n",
    "        parts = line.split(' ')\n",
    "        business_id = parts[0]\n",
    "        photo_ids = parts[1:]\n",
    "\n",
    "        if photo_ids[0] == '\\n':\n",
    "            no_photos_item += 1\n",
    "\n",
    "    print(f'{no_photos_item} business without any photo')\n",
    "    print(f'occupy {no_photos_item/total_item*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 [CLIP-ViT](https://huggingface.co/openai/clip-vit-base-patch32) 对图像进行编码\n",
    "\n",
    "- [tutorial 1](https://medium.com/@highsunday0630/image-embedding-1-clip%E6%A8%A1%E5%9E%8B%E6%8F%90%E5%8F%96-image-embedding-%E4%B8%A6%E4%BB%A5-tensorboard-%E8%A6%96%E8%A6%BA%E5%8C%96%E6%95%88%E6%9E%9C-dc281370d7d8)\n",
    "- [tutorial 2](https://blog.csdn.net/qq_37756660/article/details/135979873)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from safetensors.torch import save_file\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "import os\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/yzh/code/SELFRec/model/clip-vit-base-patch32\"\n",
    "# 图片存储路径\n",
    "directory = '/nvme0n1p2/yelp_photos/photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total record photos: 190902\n"
     ]
    }
   ],
   "source": [
    "photos = set()  # 所有需要处理的图片\n",
    "with open('yelp_out/item2photos.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        for photo_id in line.strip().split(' ')[1:]:\n",
    "            photos.add(photo_id)\n",
    "print('total record photos:', len(photos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yelp 数据集中的 photo 数据存在脏数据(无法识别、损坏)\n",
    "\n",
    "> 我是没想到公开数据集连这个都不处理好😅\n",
    "\n",
    "先写个日志看看有多少脏数据吧（手可别抖，下面的日志模块代码切记执行一次，这不比别的，多执行几次会多出几个logger，到时候你的日志里都是重复的信息）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "photo_logger = logging.getLogger('error_photo')\n",
    "photo_logger.setLevel(logging.INFO)\n",
    "sh = logging.FileHandler('../log/error_photo.log')\n",
    "sh.setLevel(logging.WARNING)\n",
    "formatter = logging.Formatter('%(name)s - %(message)s')\n",
    "sh.setFormatter(formatter)\n",
    "photo_logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "200098it [27:28, 121.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 190902 photos, processed: 190797, errors: 105, len of photo_embs: 190797\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "# model = CLIPModel.from_pretrained(model_path).to(f'cuda:{device_ids[0]}')\n",
    "model = CLIPModel.from_pretrained(model_path).to('cuda')\n",
    "processor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "# 定义批量大小\n",
    "batch_size = 64\n",
    "\n",
    "# 初始化计数器\n",
    "photo_num = 0\n",
    "processed_num = 0\n",
    "error_num = 0\n",
    "photo_embs = {}\n",
    "\n",
    "# 读取目录中的图片\n",
    "with os.scandir(directory) as entries:\n",
    "    images = []  # 待处理图片列表\n",
    "    photo_ids = []  # 图片ID列表\n",
    "\n",
    "    # 遍历目录中的所有文件\n",
    "    for entry in tqdm.tqdm(entries):\n",
    "        photo_id = entry.name.replace('.jpg', '')\n",
    "        if photo_id not in photos:\n",
    "            continue\n",
    "        photo_num += 1\n",
    "        try:\n",
    "            images.append(Image.open(entry.path))\n",
    "            photo_ids.append(photo_id)\n",
    "        except Exception as e:\n",
    "            photo_logger.warning(f'{entry.name} error: {e}')\n",
    "            error_num += 1\n",
    "            continue\n",
    "\n",
    "        # 当达到批量大小时，处理这批图片\n",
    "        if len(images) == batch_size:\n",
    "            inputs = processor(images=images, return_tensors='pt')\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                # 使用get_image_features获取图像特征\n",
    "                outputs = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "            for idx, pid in enumerate(photo_ids):\n",
    "                photo_embs[pid] = outputs[idx]\n",
    "            processed_num += batch_size\n",
    "            \n",
    "            # 清空列表\n",
    "            images = []\n",
    "            photo_ids = []\n",
    "\n",
    "    # 处理剩余的图片\n",
    "    if images:\n",
    "        inputs = processor(images=images, return_tensors='pt')\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            # 使用get_image_features获取图像特征\n",
    "            outputs = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "        for index, pid in enumerate(photo_ids):\n",
    "            photo_embs[pid] = outputs[index]\n",
    "        processed_num += len(images)\n",
    "\n",
    "print(f'total {photo_num} photos, processed: {processed_num}, errors: {error_num}, len of photo_embs: {len(photo_embs)}')\n",
    "# 保存为文件\n",
    "save_file(photo_embs, 'photo_embs.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "100.0% photos are in the dataset\n"
     ]
    }
   ],
   "source": [
    "with safe_open('photo_embs.safetensors', framework='pt') as f:\n",
    "    total_num = len(f.keys())\n",
    "    photo_num = 0\n",
    "    for k in f.keys():\n",
    "        if k in photos: photo_num += 1\n",
    "    for k in f.keys():\n",
    "        print(f.get_tensor(k).shape)\n",
    "        break\n",
    "    print(f'{photo_num/total_num*100}% photos are in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照原来的方案，有105张损坏图像，现在从映射数据 `item2photots.txt` 中删除无效图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ids = set()  # 错误图像id集合\n",
    "with open('../log/error_photo.log', 'r') as f:\n",
    "    for line in f:\n",
    "        photo_id = line.strip().split(' - ')[1].split(' ')[0].replace('.jpg', '')\n",
    "        if photo_id not in error_ids:\n",
    "            error_ids.add(photo_id)\n",
    "        else:\n",
    "            print(f'exist duplicate error photo: {photo_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWs1xspH1d-NCIWmXM40RQ remain no photo😭\n",
      "qJKMyChtpyqPvf7HNfvq4A remain no photo😭\n",
      "vpkCctZV4_q7iUkmtdZkzQ remain no photo😭\n",
      "dBa7aJXV50TZEtInwdbvfg remain no photo😭\n",
      "LIh33t2G-y0C1H3o41xJSQ remain no photo😭\n",
      "djn6PlsuFw_Z_gRA55QDcg remain no photo😭\n",
      "a1Bd6IhR_Bsthhff9VGLoA remain no photo😭\n"
     ]
    }
   ],
   "source": [
    "error_items = set()\n",
    "with open('yelp_out/re_item2photos.txt', 'w') as out_file:\n",
    "    with open('yelp_out/item2photos.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            new_line = []\n",
    "            item_id = line.strip().split(' ')[0]\n",
    "            photo_ids = line.strip().split(' ')[1:]\n",
    "            filter_ids = [id for id in photo_ids if id not in error_ids]\n",
    "            if len(filter_ids) == 0:\n",
    "                print(f'{item_id} remain no photo😭')\n",
    "                error_items.add(item_id)\n",
    "                continue\n",
    "            new_line.append(item_id)\n",
    "            new_line.extend(filter_ids)\n",
    "            out_file.write(' '.join(new_line) + '\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_out/re_item2photos.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        item = line.strip().split(' ')[0]\n",
    "        photos = line.strip().split(' ')[1:]\n",
    "        if len(photos) == 0:\n",
    "            print(f'{item} has no photo')\n",
    "        for photo in photos:\n",
    "            if photo in error_ids:\n",
    "                print(f'{item} has error photo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后从交互数据中删除 过滤损毁图像后 无剩余图像的 item 对应的交互记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/filter_yelp_interactions.txt', 'w') as out_file:\n",
    "    with open('yelp_out/re_yelp_interactions.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            item_id = line.strip().split(' ')[1]\n",
    "            if item_id in error_items:\n",
    "                print(f'delete error item {item_id}')\n",
    "                continue\n",
    "            out_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 返回 Step6 重新划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据重复行数: 0\n",
      "训练集大小: 1429113\n",
      "验证集大小: 204159\n",
      "测试集大小: 408318\n",
      "训练集重复行数: 0\n",
      "验证集重复行数: 0\n",
      "测试集重复行数: 0\n",
      "训练集、验证集和测试集之间重复行数: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('yelp_out/filter_yelp_interactions.txt', delimiter=' ', header=None)\n",
    "\n",
    "# 检查重复行\n",
    "print(f\"原始数据重复行数: {data.duplicated().sum()}\")\n",
    "\n",
    "# 划分临时集和测试集，比例为8:2\n",
    "temp_data, test_data = train_test_split(data, test_size=0.2, random_state=114514)\n",
    "# 将临时集划分为训练集和验证集，比例为7:1\n",
    "train_data, val_data = train_test_split(temp_data, test_size=0.125, random_state=114514)\n",
    "\n",
    "os.makedirs('yelp_ds', exist_ok=True)\n",
    "temp_data.to_csv('yelp_ds/merge_train_data.txt', index=False, header=False, sep=' ')\n",
    "train_data.to_csv('yelp_ds/train_data.txt', index=False, header=False, sep=' ')\n",
    "val_data.to_csv('yelp_ds/val_data.txt', index=False, header=False, sep=' ')\n",
    "test_data.to_csv('yelp_ds/test_data.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "# 查看划分后的数据集大小\n",
    "print(f\"训练集大小: {len(train_data)}\")\n",
    "print(f\"验证集大小: {len(val_data)}\")\n",
    "print(f\"测试集大小: {len(test_data)}\")\n",
    "\n",
    "# 检查是否有重复项\n",
    "print(f\"训练集重复行数: {train_data.duplicated().sum()}\")\n",
    "print(f\"验证集重复行数: {val_data.duplicated().sum()}\")\n",
    "print(f\"测试集重复行数: {test_data.duplicated().sum()}\")\n",
    "\n",
    "# 检查交叉重复项\n",
    "duplicates_in_all = train_data.merge(val_data, how='inner').merge(test_data, how='inner')\n",
    "print(f\"训练集、验证集和测试集之间重复行数: {duplicates_in_all.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "## 生成 RAG 数据\n",
    "\n",
    "从 business 数据中提取，以最终交互数据为过滤条件，预计就是下面这样\n",
    "```\n",
    "business_id categories\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total items: 33183\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "yelp_dataset_path = '/nvme0n1p2/yelp_dataset/'\n",
    "\n",
    "items = set()\n",
    "item_categories = {}\n",
    "with open('yelp_out/filter_yelp_interactions.txt', 'r') as ui_file:\n",
    "    for line in ui_file: items.add(line.split(' ')[1].strip())\n",
    "    print(f'total items: {len(items)}')\n",
    "    with open(yelp_dataset_path + 'yelp_academic_dataset_business.json', 'r') as f:\n",
    "        for line in f:\n",
    "            json_data = json.loads(line)\n",
    "            business_id = json_data['business_id']\n",
    "            categories = json_data['categories']\n",
    "            if business_id in items: item_categories[business_id] = categories\n",
    "\n",
    "# 保存item_categories到文件\n",
    "with open('yelp_out/yelp_text.json', 'w', encoding='utf-8') as text_file:\n",
    "    json.dump(item_categories, text_file, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33183\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/yelp_text.json', 'r', encoding='utf-8') as text_file:\n",
    "    data = json.load(text_file)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 LLM 增强交互数据\n",
    "\n",
    "### RAG增强(deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from pprint import pprint\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"RAG/yelp_rag.json\",\n",
    "    jq_schema=\"{item_id: .business_id, categories: .categories}\",\n",
    "    json_lines=True,\n",
    "    text_content=False\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 1}, page_content='{\"item_id\": \"MTSW4McQd7CbVtyjqoe9mw\", \"categories\": \"Restaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 2}, page_content='{\"item_id\": \"bBDDEgkFA1Otx9Lfe7BZUQ\", \"categories\": \"Ice Cream & Frozen Yogurt, Fast Food, Burgers, Restaurants, Food\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 3}, page_content='{\"item_id\": \"eEOYSgkmpB90uNA7lDOMRA\", \"categories\": \"Vietnamese, Food, Restaurants, Food Trucks\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 4}, page_content='{\"item_id\": \"il_Ro8jwPlHresjw9EGmBg\", \"categories\": \"American (Traditional), Restaurants, Diners, Breakfast & Brunch\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 5}, page_content='{\"item_id\": \"0bPLkL0QhhPO5kt1_EXmNQ\", \"categories\": \"Food, Delis, Italian, Bakeries, Restaurants\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 6}, page_content='{\"item_id\": \"MUTTqe8uqyMdBl186RmNeA\", \"categories\": \"Sushi Bars, Restaurants, Japanese\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 7}, page_content='{\"item_id\": \"ROeacJQwBeh05Rqg7F6TCg\", \"categories\": \"Korean, Restaurants\"}')\n"
     ]
    }
   ],
   "source": [
    "for idx, d in enumerate(data):\n",
    "    pprint(d)\n",
    "    if idx > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的 Document 足够小了，所以无需 split to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 实例化嵌入模型\n",
    "ollama_emb = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"qwen2\", num_gpu=2, show_progress=True)\n",
    "\n",
    "# 使用文档块创建向量数据库并持久化\n",
    "persist_directory = \"RAG\"\n",
    "# If a persist_directory is specified, the collection will be persisted there. Otherwise, the data will be ephemeral in-memory.\n",
    "vector_db = Chroma.from_documents(documents=data, \n",
    "                                     embedding=ollama_emb, \n",
    "                                     persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 实例化嵌入模型\n",
    "ollama_emb = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"qwen2\", num_gpu=2, show_progress=True)\n",
    "\n",
    "# 使用文档块创建向量数据库并持久化\n",
    "persist_directory = \"RAG\"\n",
    "# If a persist_directory is specified, the collection will be persisted there. Otherwise, the data will be ephemeral in-memory.\n",
    "vector_db = Chroma.from_documents(documents=data, \n",
    "                                     embedding=ollama_emb, \n",
    "                                     persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "\n",
    "template = \"\"\"User had interacted with the following businesses:\n",
    "{item_list}\n",
    "\n",
    "The categories of these businesses are in {context}. Please output the business_id only from the following candidate, but not user history.\n",
    "{candidates}\n",
    "\n",
    "Output format:\n",
    "business_ids separated by ','. Nothing else. Please do not output other thing else, do not give reasoning.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2\")\n",
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"item_list\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "        \"candidates\": RunnablePassthrough()\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1051810/2163359439.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  retriever.get_relevant_documents('e4Vwtrqf-wpJfwesgvdgxQ')\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'seq_num': 32384, 'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json'}, page_content='{\"item_id\": \"QKtbbF5-qny5h90Qs3erXw\", \"categories\": \"Indian, Restaurants\"}'),\n",
       " Document(metadata={'seq_num': 14523, 'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json'}, page_content='{\"item_id\": \"Qz6-OxFp9PhGwMZG5geqpw\", \"categories\": \"Italian, American (New), Restaurants\"}'),\n",
       " Document(metadata={'seq_num': 1677, 'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json'}, page_content='{\"item_id\": \"QlI4_BHwxb5UplGwd4vE0w\", \"categories\": \"Mexican, Restaurants\"}'),\n",
       " Document(metadata={'seq_num': 22024, 'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json'}, page_content='{\"item_id\": \"Q46KberieM6ziVYME6CHEQ\", \"categories\": \"Mexican, Restaurants\"}')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents('e4Vwtrqf-wpJfwesgvdgxQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = []\n",
    "candidates = []\n",
    "with open('yelp_out/filter_yelp_interactions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split(' ')[0].strip() == 'bcjbaE6dDog4jkNY91ncLQ':\n",
    "            item_list.append(line.split(' ')[1].strip())\n",
    "        else:  # 非交互项\n",
    "            if len(candidates) <= 5:\n",
    "                candidates.append(line.split(' ')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:03<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='User had interacted with the following businesses:\\n{\\'item_list\\': [\\'e4Vwtrqf-wpJfwesgvdgxQ\\', \\'gUyfJlJRxu1fHuZ4dpBheQ\\', \\'5AOkxsg6UJQ_CoJTMBDUmQ\\', \\'xkYOPbA8AL4jcQIN3xveoQ\\', \\'yJ2ZRXx01eF40eRQFqIBeQ\\', \\'ZyOqGKdr5JetY4jgD_UoGw\\', \\'D73evJ9PZKxO3E5TaThe3w\\', \\'ew5TyXOlyCpCRptye1LdxA\\', \\'q4aAaxdN4wmUZoC6sKEwsw\\', \\'4WdDY97x4GdMYtyk1KQMnw\\', \\'pym7c6ZFEtmoH16xN2ApBg\\', \\'mhrW9O0O5hXGXGnEYBVoag\\', \\'fIBMKVl-dyb3KyM11UBJPQ\\', \\'7gtWQMLOEwCxh1I5j6uB4g\\', \\'qt_E6txwQ1h62wyv8701UQ\\', \\'UjFLIhKTOiFcQiziOA9rgA\\', \\'URxxeb2R60AH81IcuxJAvQ\\', \\'HIomEsnJRxw0861yD87Qgw\\', \\'f9H3wpzWG_apxoumWB-Dvg\\', \\'al3Ri6TEqa2rBzjHsn0T_g\\', \\'mRpk0A4u0hnF0lNe1h4hGg\\', \\'dul6XjaCh1GgA-YHpuChUg\\', \\'5OpNE-GEP1unD89k61XbVQ\\', \\'c-Drp2IuAXSqjvyzvOPBzQ\\', \\'N1we1YLrBxPOoenxJwzdOA\\', \\'Z4PF4EtM12L7nwOHZHFJNA\\', \\'oFbwMxqaCJfIzAEmwaXD3Q\\', \\'uJITgt5t7j-KpDChsXPV5w\\', \\'SjgRHmQ_ClUlECE2JkY8ng\\', \\'Hj_-qd7KyQPRqTWzWgsFag\\', \\'hLr6cRTANzEll1hGlmbgHA\\', \\'nHsoeVL1dXs9ZNjmdrlPuA\\', \\'ecI3FBTM0f99Fnml3kNKfg\\', \\'8kh6Z3c8UHQKmsy0_TbOnA\\', \\'RQpOPNHJReRnrsCD-2qEoA\\', \\'jKTWcdyXPw_cGUp9fKqapQ\\', \\'Se9CEgJEVxcWax1fStWuQA\\', \\'R46XVcmUzy8qeerHyZQtEg\\', \\'dN7AMKUhwTa4Kk0bdMRPgA\\', \\'BFihjoRdU-jmdbvIEqEsxQ\\', \\'3S-u4euLhybQzOuaTAZOpg\\', \\'SkjUwG0FerzrxnIV8N56CA\\', \\'5HZPNcMR5dHQ1OyOb-RDgw\\', \\'Q9GU2OvZObDVyA00ZJkFaA\\', \\'cTSczU-9-cYUEM2DlNJcQw\\', \\'qjP2XXjtLdlZ20SISqtAAA\\', \\'vvOzblHBA2HHsCb7CMSDbQ\\', \\'ncI768qIjMFnMwYMppB4tw\\', \\'2DTkzhmMpv5fIPKheePClA\\', \\'4vak1jxwM6dQ-pNQQ5U8Vw\\', \\'RKfpN_TqD3wa58kgvnR1lQ\\', \\'8UPv1p9GW-BiZtQqUt8nOA\\', \\'ZfzTw5exIOalHsGlK99y-w\\', \\'dju1isgEvDd74tLTDkk5DA\\', \\'dGeXdSMah56gEHwZNaRQKA\\', \\'7H1b6TZ-LNxyGx1cv9suJQ\\', \\'StqG4cdKhTHmGyS7PSimdA\\', \\'if57kE6_VfR1nI1X93oHEA\\', \\'hJTwBhYBTkiHaDMml_v_sw\\', \\'TUTQeLjq1UbkR5r8mOvMqw\\', \\'PKZwdGTapRvFsBYh0zQXpw\\', \\'2x4atI8B9Z0g61bgEOO2Uw\\', \\'WXgV2lOUgas7DzTLeDau-w\\', \\'F3b3-mmClvVPUT0WvK_guA\\', \\'JjUNJCyGQlCxMwOJ9OLdiA\\', \\'EmJF-xSIOaEEOWcK8UOBqg\\', \\'4nrZ42MbdzstpNZaB_Fmew\\', \\'Xar9BJ3iIC9bOQjhXvMYqw\\', \\'HUcjSm1Lxdn8zSu-hQ7-sw\\', \\'8odXL99ki7fi2YPgvAd14w\\', \\'qm17m5Qp7nOQ3meEESLyGw\\', \\'R6P3KSFafpKz6bEtHsWz1A\\'], \\'candidates\\': [\\'RZtGWDLCAtuipwaZ-UfjmQ\\', \\'otQS34_MymijPTdNBoBdCw\\', \\'rBdG_23USc7DletfZ11xGA\\', \\'eFvzHawVJofxSnD7TgbZtg\\', \\'nRKndeZLQ3eDL10UMwS2rQ\\', \\'ut6fi2W2YaipNOqvi7e0jw\\']}\\n\\nThe categories of these businesses are in [Document(metadata={\\'seq_num\\': 6297, \\'source\\': \\'/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json\\'}, page_content=\\'{\"item_id\": \"WvPIOVr2dcUXcTWoloFmTw\", \"categories\": \"Food, Specialty Food, Gluten-Free, Vegan, Pizza, Restaurants\"}\\'), Document(metadata={\\'seq_num\\': 20357, \\'source\\': \\'/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json\\'}, page_content=\\'{\"item_id\": \"cSaoPFiZUzaTgnxM91KgXg\", \"categories\": \"Food, Restaurants, Vegan, Food Delivery Services\"}\\'), Document(metadata={\\'seq_num\\': 29629, \\'source\\': \\'/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json\\'}, page_content=\\'{\"item_id\": \"lRj_1nDUCdXS3LVSrg9vfQ\", \"categories\": \"Specialty Food, Ethnic Food, Grocery, Japanese, Food, Restaurants\"}\\'), Document(metadata={\\'seq_num\\': 9933, \\'source\\': \\'/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json\\'}, page_content=\\'{\"item_id\": \"wc2MyaDzu8Wq_4DQVMrjSA\", \"categories\": \"Health Markets, Organic Stores, Grocery, Food, Specialty Food\"}\\')]. Based on user\\'s history, please output the business_id only from the following candidate, but not user history.\\n{\\'item_list\\': [\\'e4Vwtrqf-wpJfwesgvdgxQ\\', \\'gUyfJlJRxu1fHuZ4dpBheQ\\', \\'5AOkxsg6UJQ_CoJTMBDUmQ\\', \\'xkYOPbA8AL4jcQIN3xveoQ\\', \\'yJ2ZRXx01eF40eRQFqIBeQ\\', \\'ZyOqGKdr5JetY4jgD_UoGw\\', \\'D73evJ9PZKxO3E5TaThe3w\\', \\'ew5TyXOlyCpCRptye1LdxA\\', \\'q4aAaxdN4wmUZoC6sKEwsw\\', \\'4WdDY97x4GdMYtyk1KQMnw\\', \\'pym7c6ZFEtmoH16xN2ApBg\\', \\'mhrW9O0O5hXGXGnEYBVoag\\', \\'fIBMKVl-dyb3KyM11UBJPQ\\', \\'7gtWQMLOEwCxh1I5j6uB4g\\', \\'qt_E6txwQ1h62wyv8701UQ\\', \\'UjFLIhKTOiFcQiziOA9rgA\\', \\'URxxeb2R60AH81IcuxJAvQ\\', \\'HIomEsnJRxw0861yD87Qgw\\', \\'f9H3wpzWG_apxoumWB-Dvg\\', \\'al3Ri6TEqa2rBzjHsn0T_g\\', \\'mRpk0A4u0hnF0lNe1h4hGg\\', \\'dul6XjaCh1GgA-YHpuChUg\\', \\'5OpNE-GEP1unD89k61XbVQ\\', \\'c-Drp2IuAXSqjvyzvOPBzQ\\', \\'N1we1YLrBxPOoenxJwzdOA\\', \\'Z4PF4EtM12L7nwOHZHFJNA\\', \\'oFbwMxqaCJfIzAEmwaXD3Q\\', \\'uJITgt5t7j-KpDChsXPV5w\\', \\'SjgRHmQ_ClUlECE2JkY8ng\\', \\'Hj_-qd7KyQPRqTWzWgsFag\\', \\'hLr6cRTANzEll1hGlmbgHA\\', \\'nHsoeVL1dXs9ZNjmdrlPuA\\', \\'ecI3FBTM0f99Fnml3kNKfg\\', \\'8kh6Z3c8UHQKmsy0_TbOnA\\', \\'RQpOPNHJReRnrsCD-2qEoA\\', \\'jKTWcdyXPw_cGUp9fKqapQ\\', \\'Se9CEgJEVxcWax1fStWuQA\\', \\'R46XVcmUzy8qeerHyZQtEg\\', \\'dN7AMKUhwTa4Kk0bdMRPgA\\', \\'BFihjoRdU-jmdbvIEqEsxQ\\', \\'3S-u4euLhybQzOuaTAZOpg\\', \\'SkjUwG0FerzrxnIV8N56CA\\', \\'5HZPNcMR5dHQ1OyOb-RDgw\\', \\'Q9GU2OvZObDVyA00ZJkFaA\\', \\'cTSczU-9-cYUEM2DlNJcQw\\', \\'qjP2XXjtLdlZ20SISqtAAA\\', \\'vvOzblHBA2HHsCb7CMSDbQ\\', \\'ncI768qIjMFnMwYMppB4tw\\', \\'2DTkzhmMpv5fIPKheePClA\\', \\'4vak1jxwM6dQ-pNQQ5U8Vw\\', \\'RKfpN_TqD3wa58kgvnR1lQ\\', \\'8UPv1p9GW-BiZtQqUt8nOA\\', \\'ZfzTw5exIOalHsGlK99y-w\\', \\'dju1isgEvDd74tLTDkk5DA\\', \\'dGeXdSMah56gEHwZNaRQKA\\', \\'7H1b6TZ-LNxyGx1cv9suJQ\\', \\'StqG4cdKhTHmGyS7PSimdA\\', \\'if57kE6_VfR1nI1X93oHEA\\', \\'hJTwBhYBTkiHaDMml_v_sw\\', \\'TUTQeLjq1UbkR5r8mOvMqw\\', \\'PKZwdGTapRvFsBYh0zQXpw\\', \\'2x4atI8B9Z0g61bgEOO2Uw\\', \\'WXgV2lOUgas7DzTLeDau-w\\', \\'F3b3-mmClvVPUT0WvK_guA\\', \\'JjUNJCyGQlCxMwOJ9OLdiA\\', \\'EmJF-xSIOaEEOWcK8UOBqg\\', \\'4nrZ42MbdzstpNZaB_Fmew\\', \\'Xar9BJ3iIC9bOQjhXvMYqw\\', \\'HUcjSm1Lxdn8zSu-hQ7-sw\\', \\'8odXL99ki7fi2YPgvAd14w\\', \\'qm17m5Qp7nOQ3meEESLyGw\\', \\'R6P3KSFafpKz6bEtHsWz1A\\'], \\'candidates\\': [\\'RZtGWDLCAtuipwaZ-UfjmQ\\', \\'otQS34_MymijPTdNBoBdCw\\', \\'rBdG_23USc7DletfZ11xGA\\', \\'eFvzHawVJofxSnD7TgbZtg\\', \\'nRKndeZLQ3eDL10UMwS2rQ\\', \\'ut6fi2W2YaipNOqvi7e0jw\\']}\\n\\nOutput format:\\nbusiness_ids separated by \\',\\'. Nothing else. Please do not output other thing else, do not give reasoning.\\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_chain = setup_and_retrieval | prompt\n",
    "output = test_chain.invoke({\n",
    "    \"item_list\": item_list,\n",
    "    \"candidates\": candidates\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:05<00:00,  5.68s/it]\n"
     ]
    }
   ],
   "source": [
    "chain = setup_and_retrieval | prompt | model | StrOutputParser()\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"item_list\": item_list,\n",
    "    \"candidates\": candidates\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tq4dHsAaxAXSqjvyzvOPBzQ, N1we1YLrBxPOoenxJwzdOA, Z4PF4EtM12L7nwOHZHFJNA, oFbwMxqaCJfIzAEmwaXD3Q, uJITgt5t7j-KpDChsXPV5w, SjgRHmQ_ClUlECE2JkY8ng, Hj_-qd7KyQPRqTWzWgsFag, hLr6cRTANzEll1hGlmbgHA, nHsoeVL1dXs9ZNjmdrlPuA, ecI3FBTM0f99Fnml3kNKfg, 8kh6Z3c8UHQKmsy0_TbOnA, RQpOPNHJReRnrsCD-2qEoA, jKTWcdyXPw_cGUp9fKqapQ, Se9CEgJEVxcWax1fStWuQA, R46XVcmUzy8qeerHyZQtEg, dN7AMKUhwTa4Kk0bdMRPgA, BFihjoRdU-jmdbvIEqEsxQ, 3S-u4euLhybQzOuaTAZOpg, SkjUwG0FerzrxnIV8N56CA, 5HZPNcMR5dHQ1OyOb-RDgw, Q9GU2OvZObDVyA00ZJkFaA, cTSczU-9-cYUEM2DlNJcQw, qjP2XXjtLdlZ20SISqtAAA, vvOzblHBA2HHsCb7CMSDbQ, ncI768qIjMFnMwYMppB4tw, 2DTkzhmMpv5fIPKheePClA, 4vak1jxwM6dQ-pNQQ5U8Vw, RKfpN_TqD3wa58kgvnR1lQ, 8UPv1p9GW-BiZtQqUt8nOA, ZfzTw5exIOalHsGlK99y-w, dju1isgEvDd74tLTDkk5DA, dGeXdSMah56gEHwZNaRQKA, 7H1b6TZ-LNxyGx1cv9suJQ, StqG4cdKhTHmGyS7PSimdA, if57kE6_VfR1nI1X93oHEA, hJTwBhYBTkiHaDMml_v_sw, TUTQeLjq1UbkR5r8mOvMqw, PKZwdGTapRvFsBYh0zQXpw, 2x4atI8B9Z0g61bgEOO2Uw, WXgV2lOUgas7DzTLeDau-w, F3b3-mmClvVPUT0WvK_guA, JjUNJCyGQlCxMwOJ9OLdiA, EmJF-xSIOaEEOWcK8UOBqg, 4nrZ42MbdzstpNZaB_Fmew, Xar9BJ3iIC9bOQjhXvMYqw, HUcjSm1Lxdn8zSu-hQ7-sw, 8odXL99ki7fi2YPgvAd14w, qm17m5Qp7nOQ3meEESLyGw, R6P3KSFafpKz6bEtHsWz1A\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这么看 RAG 效果并不好啊。。。而且文本内容太短了，效率远不如直接拼接 prompt，还浪费我几个小时做 embedding。。。焯\n",
    "\n",
    "不过 LangChain 的辅助功能还是能用，比如模板之类的，比手搓要强\n",
    "\n",
    "### prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_rag = {}\n",
    "with open('RAG/yelp_rag.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        yelp_rag[data['business_id']] = data['categories']        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('MTSW4McQd7CbVtyjqoe9mw', 'Restaurants, Food, Bubble Tea, Coffee & Tea, Bakeries')\n",
      "1 ('bBDDEgkFA1Otx9Lfe7BZUQ', 'Ice Cream & Frozen Yogurt, Fast Food, Burgers, Restaurants, Food')\n",
      "2 ('eEOYSgkmpB90uNA7lDOMRA', 'Vietnamese, Food, Restaurants, Food Trucks')\n",
      "3 ('il_Ro8jwPlHresjw9EGmBg', 'American (Traditional), Restaurants, Diners, Breakfast & Brunch')\n",
      "4 ('0bPLkL0QhhPO5kt1_EXmNQ', 'Food, Delis, Italian, Bakeries, Restaurants')\n"
     ]
    }
   ],
   "source": [
    "for idx, (k, v) in enumerate(yelp_rag.items()):\n",
    "    print(idx, (k, v))\n",
    "    if idx > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import  StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = Ollama(\n",
    "    #base_url=\"http://172.16.110.34:45665\",  # node09\n",
    "    base_url=\"http://localhost:11434\",  # whr\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    num_gpu=2,\n",
    "    # It is recommended to set this value to the number of physical CPU cores\n",
    "    num_thread=48,\n",
    "    # system prompt (overrides what is defined in the Modelfile)\n",
    "    system='You are a recommendation system and required to recommend user with businesses based on user history that each business with id, categories.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"User had interacted with the [history] businesses. Output user's favorite and least favorite business_ids from the [candidates], not from the user [history]. Each line is \"business_id: categories\". Output format are two business_ids separated by ','. The first is favorite and the second is least favorite. No any other things. No reasoning.\n",
    "[history]\n",
    "{item_list}\n",
    "\n",
    "[candidates]\n",
    "{candidates}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = RunnablePassthrough() | prompt | ollama_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = \"\"\"Answer the following question directly without reasoning:\n",
    "{question}\n",
    "\"\"\"\n",
    "test_prompt = ChatPromptTemplate.from_template(test_template)\n",
    "test_chain = RunnablePassthrough() | test_prompt | ollama_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成模板数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4Vwtrqf-wpJfwesgvdgxQ: Sandwiches, Beer, Wine & Spirits, Bars, Food, Restaurants, American (Traditional), Nightlife\n",
      "RZtGWDLCAtuipwaZ-UfjmQ: Pizza, Restaurants, Italian, Salad\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "item_list = []\n",
    "candidates = []\n",
    "\n",
    "# 生成id数据\n",
    "with open('yelp_out/filter_yelp_interactions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split(' ')[0].strip() == 'bcjbaE6dDog4jkNY91ncLQ':\n",
    "            # 以这个user为例，找出所有交互的items\n",
    "            item_list.append(line.split(' ')[1].strip())\n",
    "        else:  # 非交互项，取前20个试试\n",
    "            if len(candidates) < 3 :\n",
    "                candidates.append(line.split(' ')[1].strip())\n",
    "\n",
    "# 添加categories数据\n",
    "categories = {}\n",
    "with open('yelp_out/yelp_rag.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        categories[data['business_id']] = data['categories']\n",
    "\n",
    "item_list = [f'{item}: {categories[item]}' for item in item_list]\n",
    "filter_item = item_list[:3]\n",
    "candidates = [f'{item}: {categories[item]}' for item in candidates]\n",
    "\n",
    "print(item_list[0])\n",
    "print(candidates[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item_list 应该是个多行字符串，分别为 id 和 categories。candidates 同理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.11 is bigger than 9.9.\n"
     ]
    }
   ],
   "source": [
    "t_output = test_chain.invoke({\n",
    "    \"question\": 'Which number is bigger between 9.9 and 9.11?',\n",
    "})\n",
    "print(t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: User had interacted with the [history] businesses. Output user's favorite and least favorite business_ids from the [candidates], not from the user [history]. Each line is \"business_id: categories\". Output format are two business_ids separated by ','. The first is favorite and the second is least favorite. No any other things. No reasoning.\n",
      "[history]\n",
      "e4Vwtrqf-wpJfwesgvdgxQ: Sandwiches, Beer, Wine & Spirits, Bars, Food, Restaurants, American (Traditional), Nightlife\n",
      "gUyfJlJRxu1fHuZ4dpBheQ: Mexican, Restaurants, Latin American\n",
      "5AOkxsg6UJQ_CoJTMBDUmQ: American (New), Restaurants, Cajun/Creole\n",
      "\n",
      "[candidates]\n",
      "RZtGWDLCAtuipwaZ-UfjmQ: Pizza, Restaurants, Italian, Salad\n",
      "otQS34_MymijPTdNBoBdCw: Restaurants, Tacos, Mexican, Hot Dogs, Breakfast & Brunch, Steakhouses\n",
      "rBdG_23USc7DletfZ11xGA: Wine Bars, Bars, Nightlife, American (New), Mediterranean, Restaurants\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_item_list = \"\\n\".join(filter_item)\n",
    "formatted_candidates = \"\\n\".join(candidates)\n",
    "prompt_chain = RunnablePassthrough() | prompt\n",
    "out_prompt = prompt_chain.invoke({\n",
    "    \"item_list\": formatted_item_list,\n",
    "    \"candidates\": formatted_candidates\n",
    "})\n",
    "print(out_prompt.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RZtGWDLCAtuipwaZ-UfjmQ, otQS34_MymijPTdNBoBdCw\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\n",
    "    \"item_list\": formatted_item_list,\n",
    "    \"candidates\": formatted_candidates\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "item_list = []\n",
    "candidates = []\n",
    "\n",
    "# 生成id数据\n",
    "with open('mk_dataset/yelp_ds_final/train_data.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split(' ')[0].strip() == 'bcjbaE6dDog4jkNY91ncLQ':\n",
    "            # 以这个user为例，找出所有交互的items\n",
    "            item_list.append(line.split(' ')[1].strip())\n",
    "        else:  # 非交互项，取前20个试试\n",
    "            if len(candidates) < 20 :\n",
    "                candidates.append(line.split(' ')[1].strip())\n",
    "\n",
    "# 添加categories数据\n",
    "categories = {}\n",
    "with open('yelp_out/yelp_rag.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        categories[data['business_id']] = data['categories']\n",
    "item_list = [f'{item}: {categories[item]}' for item in item_list]\n",
    "candidates = [f'{item}: {categories[item]}' for item in candidates]\n",
    "\n",
    "print(item_list[0])\n",
    "print(candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t 统计量: 30.792014356777727, p-value: 0.0010530218790085494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# 模型 A 和模型 B 的 Recall@20 值\n",
    "recall_A = np.array([0.1480,0.1481,0.1482])\n",
    "recall_B = np.array([0.14649,0.14650,0.14651])\n",
    "\n",
    "# 计算每次实验的差异\n",
    "differences = recall_A - recall_B\n",
    "\n",
    "# 进行配对 t 检验\n",
    "t_stat, p_value = stats.ttest_rel(recall_A, recall_B)\n",
    "\n",
    "print(f\"t 统计量: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据增强2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = Ollama(\n",
    "    #base_url=\"http://172.16.110.34:45665\",  # node09\n",
    "    base_url=\"http://localhost:11434\",  # whr\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    num_gpu=2,\n",
    "    # It is recommended to set this value to the number of physical CPU cores\n",
    "    num_thread=48,\n",
    "    # system prompt (overrides what is defined in the Modelfile)\n",
    "    system='You are a recommendation system and required to recommend user with businesses based on user history that each business with id, categories.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试一下能不能从文件读取模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User had interacted with the following businesses. Each line is \"business_id: categories\". Please summarize this user preference categories with historical information. Give answers directly. Do not output any other things. Do not give reasoning.\n",
      "\n",
      "[history]\n",
      "{history}\n",
      "\n",
      "[output format]\n",
      "categorie1, categorie2, ...\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"User had interacted with the following businesses. Each line is \"business_id: categories\". Please summarize this user preference with historical information. Give answers directly. Do not output any other things. Do not give reasoning.\n",
    "\n",
    "[history]\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "template_file = '/home/yzh/code/SELFRec/conf/aug_prompt.txt'\n",
    "with open(template_file, 'r') as f:\n",
    "    template = f.read()\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_out/yelp_text.json', 'r') as text_file:\n",
    "    yelp_text = json.load(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = RunnablePassthrough() | prompt | ollama_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e4Vwtrqf-wpJfwesgvdgxQ: Sandwiches, Beer, Wine & Spirits, Bars, Food, Restaurants, American (Traditional), Nightlife', 'gUyfJlJRxu1fHuZ4dpBheQ: Mexican, Restaurants, Latin American', '5AOkxsg6UJQ_CoJTMBDUmQ: American (New), Restaurants, Cajun/Creole']\n"
     ]
    }
   ],
   "source": [
    "item_list = []\n",
    "\n",
    "# 生成id数据\n",
    "with open('yelp_out/filter_yelp_interactions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split(' ')[0].strip() == 'bcjbaE6dDog4jkNY91ncLQ':\n",
    "            # 以这个user为例，找出所有交互的items\n",
    "            item_list.append(line.split(' ')[1].strip())\n",
    "\n",
    "item_list = [f'{item}: {yelp_text[item]}' for item in item_list]\n",
    "filter_item = item_list[:3]\n",
    "\n",
    "print(filter_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: User had interacted with the following businesses. Each line is \"business_id: categories\". Please summarize this user preference categories with historical information. Give answers directly. Do not output any other things. Do not give reasoning.\n",
      "\n",
      "[history]\n",
      "e4Vwtrqf-wpJfwesgvdgxQ: Sandwiches, Beer, Wine & Spirits, Bars, Food, Restaurants, American (Traditional), Nightlife\n",
      "gUyfJlJRxu1fHuZ4dpBheQ: Mexican, Restaurants, Latin American\n",
      "5AOkxsg6UJQ_CoJTMBDUmQ: American (New), Restaurants, Cajun/Creole\n",
      "\n",
      "[output format]\n",
      "categorie1, categorie2, ...\n"
     ]
    }
   ],
   "source": [
    "form_item_list = '\\n'.join(filter_item)\n",
    "prompt_chain = RunnablePassthrough() | prompt\n",
    "out_prompt = prompt_chain.invoke({\n",
    "    \"history\": form_item_list,\n",
    "})\n",
    "print(out_prompt.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American (Traditional), Mexican, American (New)\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\n",
    "    \"history\": form_item_list,\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并文件\n",
    "\n",
    "由于存在请求中断的情况，所以分批次产生了若干个生成文件，为不遗漏，通过id进行控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116507\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('yelp_out/yelp_user_history.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2id = {}\n",
    "id2user = {}\n",
    "for i, user in enumerate(data):\n",
    "    user2id[user] = i\n",
    "    id2user[i] = user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116449"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user2id['_fCu_7tmTX-DevfSyoyqsg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116506"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user2id['V9fW3-fJ-sEMz_ewPpzXXg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oe3JA8llbDetMWxSPjJHVA'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2user[3339]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成完毕，统计数据量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116507"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8649+30000+77800+58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-YOWyjJ0bOdSN0LfDSLC4Q': ''}\n"
     ]
    }
   ],
   "source": [
    "with open('/nvme0n1p2/yelp_out/yelp_user_preference-20240929_0832.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data: dict = json.loads(line)\n",
    "        if list(data.keys())[0] == '-YOWyjJ0bOdSN0LfDSLC4Q':\n",
    "            print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并生成文件(文件不大，就不写流了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    '/nvme0n1p2/yelp_out/yelp_user_preference-20240930_1006.json',  # 0-8648\n",
    "    '/nvme0n1p2/yelp_out/yelp_user_preference-20240928_2011.json',  # 8649-38648\n",
    "    '/nvme0n1p2/yelp_out/yelp_user_preference-20240929_0832.json',  # 38649-116448\n",
    "    '/nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json'   # 116449-116506\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8649 /nvme0n1p2/yelp_out/yelp_user_preference-20240930_1006.json\n",
      "30000 /nvme0n1p2/yelp_out/yelp_user_preference-20240928_2011.json\n",
      "77800 /nvme0n1p2/yelp_out/yelp_user_preference-20240929_0832.json\n",
      "58 /nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wc -l /nvme0n1p2/yelp_out/yelp_user_preference-20240930_1006.json\n",
    "wc -l /nvme0n1p2/yelp_out/yelp_user_preference-20240928_2011.json\n",
    "wc -l /nvme0n1p2/yelp_out/yelp_user_preference-20240929_0832.json\n",
    "wc -l /nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116507"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8649+30000+77800+58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_fCu_7tmTX-DevfSyoyqsg\": \"restaurants, bars\"}\n",
      "{\"V9fW3-fJ-sEMz_ewPpzXXg\": \"Music Venues, Bars\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head /nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json -n 1\n",
    "tail /nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json line -> json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "user_preferences: dict[str, str] = {}  # user->categories\n",
    "with open('yelp_user_preferences.v2.json', 'w', encoding='utf-8') as fout:\n",
    "    for path in file_paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data: dict[str, str] = json.loads(line)  # user->categories\n",
    "                    for k,v in data.items():  # 其实只有一组\n",
    "                        if k in user_preferences: # 应该不会有\n",
    "                            print('exist duplicate user: ', k)\n",
    "                        user_preferences[k] = v\n",
    "                except Exception as e:\n",
    "                    with open('error_preferences.txt', 'a', encoding='utf-8') as errors:\n",
    "                        errors.write(f\"{path.split('-')[1]}: {line}\")\n",
    "    \n",
    "    json.dump(user_preferences, fout, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 116507 users, 73 empty fields\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "with open('yelp_user_preferences.v1.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for k, v in data.items():\n",
    "        if v.strip() == \"\":\n",
    "            error += 1\n",
    "    print(f\"total {len(data)} users, {error} empty fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行 `python ollama_aug.py --type specific` 后，更新 `yelp_user_preferences.json`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 116507\n",
      "fix: 73\n",
      "after: 116507\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('yelp_user_preferences.v1.json', 'r') as origin:\n",
    "    origin_data: dict[str, str] = json.load(origin)\n",
    "    print(f\"before: {len(origin_data)}\")\n",
    "\n",
    "    fix_path = '/nvme0n1p2/yelp_out/yelp_user_preference-20241007_1708.json'\n",
    "    fix_data: dict[str, str] = {}\n",
    "    with open(fix_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data: dict[str, str] = json.loads(line)\n",
    "            for k,v in data.items():\n",
    "                fix_data[k] = v\n",
    "    print(f\"fix: {len(fix_data)}\")\n",
    "\n",
    "    # 更新数据\n",
    "    with open('yelp_user_preferences.v2.json', 'w') as output:\n",
    "        for k in fix_data.keys():\n",
    "            if k in origin_data:\n",
    "                origin_data[k] = fix_data[k]\n",
    "            else:\n",
    "                print(f\"{k} is in fix data but not in origin data!\")\n",
    "        json.dump(origin_data, output, ensure_ascii=False)\n",
    "    print(f\"after: {len(origin_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 116507 users, 0 empty fields\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "with open('yelp_user_preferences.v2.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for k, v in data.items():\n",
    "        if v.strip() == \"\":\n",
    "            error += 1\n",
    "    print(f\"total {len(data)} users, {error} empty fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次添加，直接构建全局的吧，反正差不了几个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 116507\n",
      "fix: 508\n",
      "after: 117015\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('yelp_out/yelp_user_preferences.v2.json', 'r') as origin:\n",
    "    origin_data: dict[str, str] = json.load(origin)\n",
    "    print(f\"before: {len(origin_data)}\")\n",
    "\n",
    "    fix_path = 'yelp_out/preferences/yelp_user_preference-20241022_2023.json'\n",
    "    fix_data: dict[str, str] = {}\n",
    "    with open(fix_path, 'r') as f:\n",
    "        data: dict[str, str] = json.load(f)\n",
    "        for k,v in data.items():\n",
    "            fix_data[k] = v\n",
    "    print(f\"fix: {len(fix_data)}\")\n",
    "\n",
    "    # 更新数据\n",
    "    with open('yelp_user_preferences.v3.json', 'w') as output:\n",
    "        for k, v in fix_data.items():\n",
    "            if k in origin_data:\n",
    "                print('exist')\n",
    "            else:\n",
    "                origin_data[k] = v\n",
    "        json.dump(origin_data, output, ensure_ascii=False)\n",
    "    print(f\"after: {len(origin_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本嵌入生成\n",
    "\n",
    "[stella_en_1.5B_v5](https://huggingface.co/bennegeek/stella_en_1.5B_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "query_prompt_name = \"s2s_query\"\n",
    "model = SentenceTransformer('/home/yzh/code/SELFRec/model/stella_en_1.5B_v5', device=\"cuda:1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成 user_pref_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "# 读取数据\n",
    "with open('yelp_out/yelp_user_preferences.v3.json', 'r', encoding='utf-8') as file:\n",
    "    user_preferences: dict[str, str] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total text: 116507\n",
      "output embedding shape: torch.Size([116507, 1024])\n"
     ]
    }
   ],
   "source": [
    "user_pre_embs: dict[str, torch.Tensor] = {}\n",
    "user_pre_list: list[str] = []\n",
    "for prefs in user_preferences.values():\n",
    "    user_pre_list.append(prefs)\n",
    "print(f'total text: {len(user_pre_list)}')\n",
    "\n",
    "# 模型编码\n",
    "pre_embs = model.encode(user_pre_list, prompt_name=query_prompt_name, device='cuda:1', batch_size=16, convert_to_tensor=True)\n",
    "print(f\"output embedding shape: {pre_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fix user pref: 100%|██████████| 117015/117015 [00:15<00:00, 7533.11it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pref_embs = load_file(\"yelp_ds/user_pre_embs.safetensors\", device=\"cuda:1\")\n",
    "for user in tqdm(user_preferences, desc='fix user pref'):\n",
    "    if user in pref_embs: continue\n",
    "    emb = model.encode(user_preferences[user], prompt_name=query_prompt_name, device='cuda:1', convert_to_tensor=True)\n",
    "    pref_embs[user] = emb\n",
    "\n",
    "metadata = {\n",
    "    \"type\": \"pt\",\n",
    "    \"user num\": \"117015\",\n",
    "    \"dim\": \"1024\",\n",
    "    \"build by\": \"https://github.com/sun2ot\",\n",
    "    \"time\": \"2024-10-22 20:51\"\n",
    "}\n",
    "save_file(pref_embs, \"user_pre_embs.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117015\n"
     ]
    }
   ],
   "source": [
    "print(len(pref_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116507\n",
      "tensor([-2.0780,  1.0266, -0.3841,  ..., -1.3006, -2.8999,  3.2373],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "loaded = load_file('user_pre_embs.safetensors', device='cuda:1')\n",
    "print(len(loaded))\n",
    "print(loaded['GziM44xJcoR4jJByq10NQA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116507\n",
      "tensor([-2.0780,  1.0266, -0.3841,  ..., -1.3006, -2.8999,  3.2373],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "with safe_open('user_pre_embs.safetensors', framework='pt', device='cuda:1') as f: # type: ignore\n",
    "    print(len(f.keys()))\n",
    "    print(f.get_tensor('GziM44xJcoR4jJByq10NQA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如法炮制, 生成 text_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total text: 33183\n",
      "output embedding shape: torch.Size([33183, 1024])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "with open('yelp_out/yelp_text.json', 'r', encoding='utf-8') as text_file:\n",
    "    item_text: dict[str, str] = json.load(text_file)\n",
    "\n",
    "item_text_embs: dict[str, torch.Tensor] = {}\n",
    "item_text_list: list[str] = []\n",
    "for text in item_text.values():\n",
    "    item_text_list.append(text)\n",
    "print(f'total text: {len(item_text_list)}')\n",
    "\n",
    "text_embs = model.encode(item_text_list, device='cuda:1', prompt_name=query_prompt_name, batch_size=32, convert_to_tensor=True)\n",
    "print(f\"output embedding shape: {text_embs.shape}\")\n",
    "\n",
    "for idx, item in enumerate(item_text.keys()):\n",
    "    item_text_embs[item] = text_embs[idx]\n",
    "save_file(item_text_embs, 'item_text_embs.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33183\n",
      "tensor([-0.8660,  2.7190, -0.9495,  ...,  1.4988, -0.4805,  2.8815],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "with safe_open('item_text_embs.safetensors', framework='pt', device='cuda:1') as f: # type: ignore\n",
    "    print(len(f.keys()))\n",
    "    print(f.get_tensor('MTSW4McQd7CbVtyjqoe9mw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 缩小数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先构建交互字典，便于后续处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "ui_dict = defaultdict(dict)\n",
    "with open('yelp_ds/filter_yelp_interactions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        user, item, ratings = line.strip().split(' ')\n",
    "        ui_dict[user][item] = ratings\n",
    "\n",
    "# 持久化\n",
    "with open('yelp_ds/ui_dict.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(ui_dict, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117015\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_ds/ui_dict.json', 'r', encoding='utf-8') as file:\n",
    "    ui_data = json.load(file)\n",
    "print(len(ui_data))\n",
    "del ui_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 117015\n",
      "del not_core_user: 79618\n",
      "users: 37397, items: 32491, it_num: 707178\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open('yelp_ds/ui_dict.json', 'r', encoding='utf-8') as file:\n",
    "    # 有评分, 浮点型, 以字符串保存\n",
    "    ui_dict: dict[str, dict[str, str]] = json.load(file)\n",
    "origin = len(ui_dict)\n",
    "print(f\"origin: {origin}\")\n",
    "\n",
    "not_core_user=0\n",
    "for user, item_ratings in list(ui_dict.items()):\n",
    "    if len(item_ratings) < 15:\n",
    "        not_core_user+=1\n",
    "        # 先删除交互不足15次的user\n",
    "        del ui_dict[user]\n",
    "print(f\"del not_core_user: {not_core_user}\")\n",
    "\n",
    "for user, item_ratings in ui_dict.items():\n",
    "    if len(item_ratings) < 10: raise Exception('exit user it < 10')\n",
    "    if len(item_ratings) > 20:\n",
    "        # 保留至多20次交互\n",
    "        select_items = random.sample(list(item_ratings.keys()), 20)\n",
    "        ui_dict[user] = {item: item_ratings[item] for item in select_items}\n",
    "\n",
    "item_set = set()\n",
    "it_num = 0\n",
    "for user, item_ratings in ui_dict.items():\n",
    "    it_num += len(item_ratings)\n",
    "    item_set.update(item_ratings.keys())\n",
    "\n",
    "print(f\"users: {len(ui_dict)}, items: {len(item_set)}, it_num: {it_num}\")\n",
    "\n",
    "\n",
    "# 持久化\n",
    "with open('yelp_tiny/ui_dict.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(ui_dict, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37397\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_tiny/ui_dict.json', 'r', encoding='utf-8') as file:\n",
    "    d = json.load(file)\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_tiny/ui_dict.json', 'r', encoding='utf-8') as file:\n",
    "    ui_dict = json.load(file)\n",
    "\n",
    "train_set = []\n",
    "val_set = []\n",
    "test_set = []\n",
    "not_core = 0\n",
    "\n",
    "for user, item_ragings in ui_dict.items():\n",
    "    items = list(item_ragings.keys())\n",
    "    random.shuffle(items)  # 打乱交互数据\n",
    "    it_num = len(items)\n",
    "    if it_num < 10: not_core+=1  # 不足10个会导致对应测试集为空\n",
    "    train_num = int(it_num * 0.7)\n",
    "    val_num = int(it_num * 0.1) + train_num\n",
    "\n",
    "    # 将每个用户的交互记录划分到三个子集\n",
    "    for item in items[:train_num]: train_set.append((user, item, item_ragings[item]))\n",
    "    for item in items[train_num:val_num]: val_set.append((user, item, item_ragings[item]))\n",
    "    for item in items[val_num:]: test_set.append((user, item, item_ragings[item]))\n",
    "print(not_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488814\n",
      "62360\n",
      "156004\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def save_dataset(dataset: list[tuple[str, str, str]], filename: str):\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        for user, item, ratings in tqdm(dataset):\n",
    "            f.write(f\"{user} {item} {ratings}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488814/488814 [00:00<00:00, 2066120.92it/s]\n",
      "100%|██████████| 62360/62360 [00:00<00:00, 1542025.35it/s]\n",
      "100%|██████████| 156004/156004 [00:00<00:00, 1704365.84it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(train_set, 'yelp_tiny/train.txt')\n",
    "save_dataset(val_set, 'yelp_tiny/val.txt')\n",
    "save_dataset(test_set, 'yelp_tiny/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 551174/551174 [00:00<00:00, 1696203.46it/s]\n"
     ]
    }
   ],
   "source": [
    "merge_train = []\n",
    "for i in train_set: merge_train.append(i)\n",
    "for j in val_set: merge_train.append(j)\n",
    "print(len(merge_train))\n",
    "save_dataset(merge_train, 'yelp_tiny/merge_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像模态二次处理\n",
    "\n",
    "yelp 图像模态数据需要再处理下，多图直接合一，避免模型过程中处理，影响代码复用性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "item images: 100%|██████████| 33183/33183 [00:09<00:00, 3548.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "import torch\n",
    "\n",
    "item2image: dict[str, list[str]] = {}\n",
    "with safe_open('yelp_ds/photo_embs.safetensors', 'pt', device=\"cuda:0\") as image_safetensors: # type: ignore\n",
    "    item_set = set()\n",
    "    with open('yelp_out/re_item2photos.txt', 'r') as map_file:\n",
    "        for line in map_file:\n",
    "            item = line.strip().split(' ')[0]\n",
    "            item_set.add(item)\n",
    "            images = line.strip().split(' ')[1:]\n",
    "            item2image[item] = images\n",
    "\n",
    "    merge_image_tensor: dict[str, torch.Tensor] = {}\n",
    "    for item in tqdm(item_set, desc='item images'):\n",
    "        # 这里是全局图像, 不是 tiny\n",
    "        try:\n",
    "            merge_image_tensor[item] = torch.mean(\n",
    "                torch.stack([image_safetensors.get_tensor(image) for image in item2image[item]]), dim=0\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            exit(-1)\n",
    "    \n",
    "    save_file(merge_image_tensor, 'yelp_ds/item_image_emb.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
