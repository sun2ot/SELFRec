{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ds_util import head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_dataset_path = '/nvme0n1p2/yelp_dataset/'\n",
    "yelp_photo_path = '/nvme0n1p2/yelp_photos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# äº¤äº’æ•°æ®\n",
    "\n",
    "## Step1ï¼šä» review æ•°æ®é›†ä¸­æå–æ‰€æœ‰ç”¨æˆ·ï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„è¯„è®ºæ•°é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ user_id ä¸ºé›†åˆ\n",
    "user_ids = set()\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_user.json', 'r', encoding='utf-8') as f_users:\n",
    "    for line in f_users:\n",
    "        user_data = json.loads(line)\n",
    "        user_ids.add(user_data['user_id'])\n",
    "\n",
    "# ç»Ÿè®¡æ¯ä¸ª user_id çš„å®é™… reviews æ•°é‡\n",
    "user_review_count = {}\n",
    "\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        user_id = review_data['user_id']\n",
    "        if user_id in user_ids:\n",
    "            if user_id in user_review_count:\n",
    "                user_review_count[user_id] += 1\n",
    "            else:\n",
    "                user_review_count[user_id] = 1\n",
    "\n",
    "# ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "with open('yelp_out/user_review_count.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for user_id, count in user_review_count.items():\n",
    "        output_file.write(f\"{user_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_-eMZ6K5RLWhZyISBhwA 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/user_review_count.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2ï¼šç­›é€‰å‡ºäº¤äº’æ¬¡æ•°ï¼ˆè¯„è®ºæ•°ï¼‰å¤§äºç­‰äº 10 æ¬¡çš„ç”¨æˆ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "items = {}\n",
    "with open('yelp_out/user_review_count.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # count_data -> [user_id, review_count]\n",
    "        count_data = line.strip().split(' ')\n",
    "        if int(count_data[1]) >= 10:\n",
    "            items[count_data[0]] = count_data[1]\n",
    "        if int(count_data[1]) < 1:\n",
    "            print(f'user {count_data[0]} has 0 review!')\n",
    "\n",
    "# ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "with open('yelp_out/core_users.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for user_id, count in items.items():\n",
    "        output_file.write(f\"{user_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_-eMZ6K5RLWhZyISBhwA 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/core_users.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3ï¼šä» review æ•°æ®é›†ä¸­æå–æ‰€æœ‰é¡¹ç›®å•†å®¶ï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªå•†å®¶çš„è¯„è®ºæ•°é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ business_id ä¸ºé›†åˆ\n",
    "business_ids = set()\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_business.json', 'r', encoding='utf-8') as f_item:\n",
    "    for line in f_item:\n",
    "        item_data = json.loads(line)\n",
    "        business_ids.add(item_data['business_id'])\n",
    "\n",
    "# ç»Ÿè®¡æ¯ä¸ª business_id çš„å®é™… reviews æ•°é‡\n",
    "item_review_count = {}\n",
    "\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        business_id = review_data['business_id']\n",
    "        if business_id in business_ids:\n",
    "            if business_id in item_review_count:\n",
    "                item_review_count[business_id] += 1\n",
    "            else:\n",
    "                item_review_count[business_id] = 1\n",
    "\n",
    "# ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "with open('yelp_out/item_review_count.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for business_id, count in item_review_count.items():\n",
    "        output_file.write(f\"{business_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQfwVwDr-v0ZS3_CbbE5Xw 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/item_review_count.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4ï¼šç­›é€‰å‡ºäº¤äº’æ¬¡æ•°å¤§äºç­‰äº 10 æ¬¡ä¸”å­˜åœ¨ photos æ•°æ®çš„å•†å®¶\n",
    "\n",
    "ç†è®ºä¸Šè¯´ï¼Œ10-core settings åªéœ€è¦äº¤äº’æ¬¡æ•°é™åˆ¶å°±å¯ä»¥äº†ã€‚ä½†æœ¬æ–‡ç”±äºéœ€è¦å›¾ç‰‡æ¨¡æ€æ•°æ®ï¼Œè€Œå®é™…å¤„ç†è¿‡ç¨‹ä¸­å‘ç°å¦‚æœä¸æ·»åŠ â€œæœ‰å›¾ç‰‡â€çš„é™åˆ¶æ¡ä»¶ï¼Œç”Ÿæˆçš„æ•°æ®é›†ä¸­ï¼Œå°†æœ‰çº¦ 67% çš„ business æ²¡æœ‰å¯¹åº”çš„ photos æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "items = {}\n",
    "business_with_photos = set()\n",
    "\n",
    "try:\n",
    "    with open(yelp_photo_path + 'photos.json', 'r') as photo_list:\n",
    "        for line in photo_list:\n",
    "            data = json.loads(line)\n",
    "            business_id = data['business_id']\n",
    "            business_with_photos.add(business_id)\n",
    "\n",
    "    with open('yelp_out/item_review_count.txt', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # count_data -> [business_id, review_count]\n",
    "            count_data = line.strip().split(' ')\n",
    "            if int(count_data[1]) >= 10 and count_data[0] in business_with_photos:\n",
    "                items[count_data[0]] = count_data[1]\n",
    "            if int(count_data[1]) < 1:\n",
    "                print(f'business {count_data[0]} has 0 review!')\n",
    "\n",
    "    # ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "    with open('yelp_out/re_core_items.txt', 'w', encoding='utf-8') as output_file:\n",
    "        for business_id, count in items.items():\n",
    "            output_file.write(f\"{business_id} {count}\\n\")\n",
    "        \n",
    "        print(\"generate complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'errors: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQfwVwDr-v0ZS3_CbbE5Xw 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/core_items.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5ï¼šäº¤å‰è¿‡æ»¤-ä» review æ•°æ®é›†ä¸­æå–å‡º 10-core äº¤äº’è®°å½•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼Œyelp æ•°æ®é›†è‡ªå¸¦æ¢è¡Œç¬¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        if '\\n' in line: \n",
    "            print('yes')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤–ï¼Œç”±äºå­˜åœ¨ä¸€ä¸ªç”¨æˆ·å¯¹åŒä¸€å•†å®¶çš„å¤šæ¡è¯„è®ºè®°å½•ï¼Œå› æ­¤æ„é€ è¿‡æ»¤ review æ•°æ®é›†æ—¶ï¼Œéœ€è€ƒè™‘ä»¥ä¸‹å‡ æ–¹é¢ï¼š\n",
    "1. ç”¨æˆ·å’Œé¡¹ç›®éƒ½æ˜¯æ ¸å¿ƒç”¨æˆ·/é¡¹ç›®\n",
    "2. (user_id ,business_id) ä¸é‡å¤\n",
    "3. é’ˆå¯¹é‡å¤çš„æƒ…å†µï¼Œå–è¯„åˆ†æœ€é«˜é¡¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter reviews done\n"
     ]
    }
   ],
   "source": [
    "core_users = set()\n",
    "items = set()\n",
    "# ä¿è¯ä¸€ä¸ªç”¨æˆ·å¯¹ä¸€ä¸ªé¡¹ç›®åªæœ‰ä¸€æ¡äº¤äº’è®°å½•\n",
    "user_item_pairs = {}\n",
    "\n",
    "with open('yelp_out/core_users.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        user_id = line.strip().split(' ')[0]\n",
    "        core_users.add(user_id)\n",
    "\n",
    "with open('yelp_out/re_core_items.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        business_id = line.strip().split(' ')[0]\n",
    "        items.add(business_id)\n",
    "\n",
    "filter_reviews = []  # -> [user_id, business_id, stars, review_id]\n",
    "index = 0\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        user_id = review_data['user_id']\n",
    "        business_id = review_data['business_id']\n",
    "\n",
    "        if user_id in core_users and business_id in items:\n",
    "            if (user_id, business_id) not in user_item_pairs:\n",
    "                # ç”¨å“ˆå¸Œè¡¨åŒæ—¶è®°å½•è¯„åˆ†å’Œç´¢å¼•ï¼Œä»¥ä¾¿åç»­æ“ä½œ\n",
    "                user_item_pairs[(user_id, business_id)] = [review_data['stars'], index]\n",
    "                # filter_reviews.append([user_id, business_id, review_data['stars'], review_data['review_id']])\n",
    "                filter_reviews.append([user_id, business_id, review_data['stars']])\n",
    "                index += 1\n",
    "            else:\n",
    "                if review_data['stars'] > user_item_pairs[(user_id, business_id)][0]:\n",
    "                    pair_index = user_item_pairs[(user_id, business_id)][1]\n",
    "                    # æ›´æ–°æ•°æ®\n",
    "                    # filter_reviews[pair_index] = [user_id, business_id, review_data['stars'], review_data['review_id']]\n",
    "                    filter_reviews[pair_index] = [user_id, business_id, review_data['stars']]\n",
    "\n",
    "# æŒä¹…åŒ–è¿‡æ»¤åçš„è¯„è®ºæ•°æ®é›†\n",
    "with open('yelp_out/re_yelp_interactions.txt', 'w', encoding='utf-8') as out_file:\n",
    "    for record in filter_reviews:\n",
    "        out_file.write(' '.join(map(str, record)) + '\\n')\n",
    "    print('Filter reviews done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcjbaE6dDog4jkNY91ncLQ e4Vwtrqf-wpJfwesgvdgxQ 4.0\n",
      "\n",
      "smOvOajNG0lS4Pq7d8g4JQ RZtGWDLCAtuipwaZ-UfjmQ 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/re_yelp_interactions.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6ï¼šåˆ’åˆ†æ•°æ®é›†\n",
    "\n",
    "å‚è€ƒ [SELFRec issue 54](https://github.com/Coder-Yu/SELFRec/issues/54)ï¼Œé¢†åŸŸå†…åœ¨å¾—åˆ°æœ€ç»ˆç»“æœæ—¶ï¼Œä¼¼ä¹ä¼šæŠŠè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆå¹¶ï¼Œæ‰€ä»¥è¿™é‡Œå…ˆæŒ‰ 8:2 åˆ’åˆ†è®­ç»ƒé›†è·Ÿæµ‹è¯•é›†ï¼Œç„¶ååœ¨å°†è®­ç»ƒé›†åˆ’åˆ†ä¸º 7:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "è®­ç»ƒé›†å¤§å°: 1429167\n",
      "æµ‹è¯•é›†å¤§å°: 612501\n",
      "è®­ç»ƒæ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "æµ‹è¯•æ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„é‡å¤è¡Œ:\n",
      "Empty DataFrame\n",
      "Columns: [0, 1, 2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('yelp_out/re_yelp_interactions.txt', delimiter=' ', header=None)\n",
    "\n",
    "# æ£€æŸ¥é‡å¤è¡Œ\n",
    "print(f\"åŸå§‹æ•°æ®é‡å¤è¡Œæ•°: {data.duplicated().sum()}\")\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸º8:2\n",
    "temp_data, test_data = train_test_split(data, test_size=0.2, random_state=114514)\n",
    "\n",
    "temp_data.to_csv('yelp_ds_re/train_data.txt', index=False, header=False, sep=' ')\n",
    "test_data.to_csv('yelp_ds_re/test_data.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "# æŸ¥çœ‹åˆ’åˆ†åçš„æ•°æ®é›†å¤§å°\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(temp_data)}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(test_data)}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤é¡¹\n",
    "print(f\"è®­ç»ƒæ•°æ®é‡å¤è¡Œæ•°: {temp_data.duplicated().sum()}\")\n",
    "print(f\"æµ‹è¯•æ•°æ®é‡å¤è¡Œæ•°: {test_data.duplicated().sum()}\")\n",
    "duplicates_in_train = temp_data.merge(test_data, how='inner')\n",
    "print(f'è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„é‡å¤è¡Œ:\\n{duplicates_in_train}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å›¾åƒæ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. æ ¹æ®ç”Ÿæˆçš„ interactions æ•°æ®ï¼Œå¯¹ç¬¬äºŒåˆ—å»ºç«‹é›†åˆï¼Œå³æ•°æ®é›†ä¸­æ‰€æœ‰ business_id\n",
    "2. å°† business_id ä»£å…¥ `photos/` æ£€ç´¢å‡ºæ‰€æœ‰ photo_id å¹¶ä¿å­˜æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å›¾åƒè®°å½•æ•°æ®å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"photo_id\": \"zsvj7vloL4L5jhYyPIuVwg\",\n",
      "  \"business_id\": \"Nk-SJhPlDBkAZvfsADtccA\",\n",
      "  \"caption\": \"Nice rock artwork everywhere and craploads of taps.\",\n",
      "  \"label\": \"inside\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "head(yelp_photo_path + 'photos.json', jsonf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å•†å®¶-å›¾ç‰‡æ˜ å°„è®°å½•\n",
    "\n",
    "ä»æœ€ç»ˆçš„**äº¤äº’æ•°æ®é›†**ä¸­ï¼Œæå–å‡ºå•†å®¶-å›¾ç‰‡(ä¸€å¯¹å¤š)çš„æ˜ å°„å…³ç³»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "photo_ids = set()\n",
    "items = set()\n",
    "item2photos = {}\n",
    "\n",
    "try:\n",
    "    with open('yelp_out/re_yelp_interactions.txt', 'r') as interact_file:\n",
    "        for line in interact_file:\n",
    "            items.add(line.split(' ')[1])\n",
    "            item2photos[line.split(' ')[1]] = []\n",
    "    \n",
    "    # item: photo_id1, photo_id2, ...\n",
    "    with open('yelp_out/item2photos.txt', 'w', encoding='utf-8') as out_file:\n",
    "        with open(yelp_photo_path + 'photos.json', 'r', encoding='utf-8') as photos:\n",
    "            for line in photos:\n",
    "                json_data = json.loads(line)\n",
    "                photo_id = json_data['photo_id']\n",
    "                business_id = json_data['business_id']\n",
    "\n",
    "                # å­˜åœ¨ photo_id é‡å¤çš„æƒ…å†µ\n",
    "                if photo_id not in photo_ids and business_id in items:\n",
    "                    item2photos[business_id].append(photo_id)\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        for item in item2photos:\n",
    "            out_file.write(item + ' ' + ' '.join(item2photos[item]) + '\\n')\n",
    "        print('generate complete')\n",
    "except Exception as e:\n",
    "    print(f'error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 business without any photo\n",
      "occupy 0.0%\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/item2photos.txt', 'r') as f:\n",
    "    no_photos_item = 0\n",
    "    total_item = 0\n",
    "\n",
    "    for line in f:\n",
    "        total_item += 1\n",
    "        parts = line.split(' ')\n",
    "        business_id = parts[0]\n",
    "        photo_ids = parts[1:]\n",
    "\n",
    "        if photo_ids[0] == '\\n':\n",
    "            no_photos_item += 1\n",
    "\n",
    "    print(f'{no_photos_item} business without any photo')\n",
    "    print(f'occupy {no_photos_item/total_item*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ [CLIP-ViT](https://huggingface.co/openai/clip-vit-base-patch32) å¯¹å›¾åƒè¿›è¡Œç¼–ç \n",
    "\n",
    "- [tutorial 1](https://medium.com/@highsunday0630/image-embedding-1-clip%E6%A8%A1%E5%9E%8B%E6%8F%90%E5%8F%96-image-embedding-%E4%B8%A6%E4%BB%A5-tensorboard-%E8%A6%96%E8%A6%BA%E5%8C%96%E6%95%88%E6%9E%9C-dc281370d7d8)\n",
    "- [tutorial 2](https://blog.csdn.net/qq_37756660/article/details/135979873)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from safetensors.torch import save_file\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "import os\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/yzh/code/SELFRec/model/clip-vit-base-patch32\"\n",
    "# å›¾ç‰‡å­˜å‚¨è·¯å¾„\n",
    "directory = '/nvme0n1p2/yelp_photos/photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total record photos: 190902\n"
     ]
    }
   ],
   "source": [
    "photos = set()  # æ‰€æœ‰éœ€è¦å¤„ç†çš„å›¾ç‰‡\n",
    "with open('yelp_out/item2photos.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        for photo_id in line.strip().split(' ')[1:]:\n",
    "            photos.add(photo_id)\n",
    "print('total record photos:', len(photos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yelp æ•°æ®é›†ä¸­çš„ photo æ•°æ®å­˜åœ¨è„æ•°æ®(æ— æ³•è¯†åˆ«ã€æŸå)\n",
    "\n",
    "> æˆ‘æ˜¯æ²¡æƒ³åˆ°å…¬å¼€æ•°æ®é›†è¿è¿™ä¸ªéƒ½ä¸å¤„ç†å¥½ğŸ˜…\n",
    "\n",
    "å…ˆå†™ä¸ªæ—¥å¿—çœ‹çœ‹æœ‰å¤šå°‘è„æ•°æ®å§ï¼ˆæ‰‹å¯åˆ«æŠ–ï¼Œä¸‹é¢çš„æ—¥å¿—æ¨¡å—ä»£ç åˆ‡è®°æ‰§è¡Œä¸€æ¬¡ï¼Œè¿™ä¸æ¯”åˆ«çš„ï¼Œå¤šæ‰§è¡Œå‡ æ¬¡ä¼šå¤šå‡ºå‡ ä¸ªloggerï¼Œåˆ°æ—¶å€™ä½ çš„æ—¥å¿—é‡Œéƒ½æ˜¯é‡å¤çš„ä¿¡æ¯ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "photo_logger = logging.getLogger('error_photo')\n",
    "photo_logger.setLevel(logging.INFO)\n",
    "sh = logging.FileHandler('../log/error_photo.log')\n",
    "sh.setLevel(logging.WARNING)\n",
    "formatter = logging.Formatter('%(name)s - %(message)s')\n",
    "sh.setFormatter(formatter)\n",
    "photo_logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "200098it [27:28, 121.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 190902 photos, processed: 190797, errors: 105, len of photo_embs: 190797\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "# model = CLIPModel.from_pretrained(model_path).to(f'cuda:{device_ids[0]}')\n",
    "model = CLIPModel.from_pretrained(model_path).to('cuda')\n",
    "processor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "# å®šä¹‰æ‰¹é‡å¤§å°\n",
    "batch_size = 64\n",
    "\n",
    "# åˆå§‹åŒ–è®¡æ•°å™¨\n",
    "photo_num = 0\n",
    "processed_num = 0\n",
    "error_num = 0\n",
    "photo_embs = {}\n",
    "\n",
    "# è¯»å–ç›®å½•ä¸­çš„å›¾ç‰‡\n",
    "with os.scandir(directory) as entries:\n",
    "    images = []  # å¾…å¤„ç†å›¾ç‰‡åˆ—è¡¨\n",
    "    photo_ids = []  # å›¾ç‰‡IDåˆ—è¡¨\n",
    "\n",
    "    # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    for entry in tqdm.tqdm(entries):\n",
    "        photo_id = entry.name.replace('.jpg', '')\n",
    "        if photo_id not in photos:\n",
    "            continue\n",
    "        photo_num += 1\n",
    "        try:\n",
    "            images.append(Image.open(entry.path))\n",
    "            photo_ids.append(photo_id)\n",
    "        except Exception as e:\n",
    "            photo_logger.warning(f'{entry.name} error: {e}')\n",
    "            error_num += 1\n",
    "            continue\n",
    "\n",
    "        # å½“è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼Œå¤„ç†è¿™æ‰¹å›¾ç‰‡\n",
    "        if len(images) == batch_size:\n",
    "            inputs = processor(images=images, return_tensors='pt')\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                # ä½¿ç”¨get_image_featuresè·å–å›¾åƒç‰¹å¾\n",
    "                outputs = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "            for idx, pid in enumerate(photo_ids):\n",
    "                photo_embs[pid] = outputs[idx]\n",
    "            processed_num += batch_size\n",
    "            \n",
    "            # æ¸…ç©ºåˆ—è¡¨\n",
    "            images = []\n",
    "            photo_ids = []\n",
    "\n",
    "    # å¤„ç†å‰©ä½™çš„å›¾ç‰‡\n",
    "    if images:\n",
    "        inputs = processor(images=images, return_tensors='pt')\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            # ä½¿ç”¨get_image_featuresè·å–å›¾åƒç‰¹å¾\n",
    "            outputs = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "        for index, pid in enumerate(photo_ids):\n",
    "            photo_embs[pid] = outputs[index]\n",
    "        processed_num += len(images)\n",
    "\n",
    "print(f'total {photo_num} photos, processed: {processed_num}, errors: {error_num}, len of photo_embs: {len(photo_embs)}')\n",
    "# ä¿å­˜ä¸ºæ–‡ä»¶\n",
    "save_file(photo_embs, 'photo_embs.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "100.0% photos are in the dataset\n"
     ]
    }
   ],
   "source": [
    "with safe_open('photo_embs.safetensors', framework='pt') as f:\n",
    "    total_num = len(f.keys())\n",
    "    photo_num = 0\n",
    "    for k in f.keys():\n",
    "        if k in photos: photo_num += 1\n",
    "    for k in f.keys():\n",
    "        print(f.get_tensor(k).shape)\n",
    "        break\n",
    "    print(f'{photo_num/total_num*100}% photos are in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŒ‰ç…§åŸæ¥çš„æ–¹æ¡ˆï¼Œæœ‰105å¼ æŸåå›¾åƒï¼Œç°åœ¨ä»æ˜ å°„æ•°æ® `item2photots.txt` ä¸­åˆ é™¤æ— æ•ˆå›¾ç‰‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ids = set()  # é”™è¯¯å›¾åƒidé›†åˆ\n",
    "with open('../log/error_photo.log', 'r') as f:\n",
    "    for line in f:\n",
    "        photo_id = line.strip().split(' - ')[1].split(' ')[0].replace('.jpg', '')\n",
    "        if photo_id not in error_ids:\n",
    "            error_ids.add(photo_id)\n",
    "        else:\n",
    "            print(f'exist duplicate error photo: {photo_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWs1xspH1d-NCIWmXM40RQ remain no photoğŸ˜­\n",
      "qJKMyChtpyqPvf7HNfvq4A remain no photoğŸ˜­\n",
      "vpkCctZV4_q7iUkmtdZkzQ remain no photoğŸ˜­\n",
      "dBa7aJXV50TZEtInwdbvfg remain no photoğŸ˜­\n",
      "LIh33t2G-y0C1H3o41xJSQ remain no photoğŸ˜­\n",
      "djn6PlsuFw_Z_gRA55QDcg remain no photoğŸ˜­\n",
      "a1Bd6IhR_Bsthhff9VGLoA remain no photoğŸ˜­\n"
     ]
    }
   ],
   "source": [
    "error_items = set()\n",
    "with open('yelp_out/re_item2photos.txt', 'w') as out_file:\n",
    "    with open('yelp_out/item2photos.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            new_line = []\n",
    "            item_id = line.strip().split(' ')[0]\n",
    "            photo_ids = line.strip().split(' ')[1:]\n",
    "            filter_ids = [id for id in photo_ids if id not in error_ids]\n",
    "            if len(filter_ids) == 0:\n",
    "                print(f'{item_id} remain no photoğŸ˜­')\n",
    "                error_items.add(item_id)\n",
    "                continue\n",
    "            new_line.append(item_id)\n",
    "            new_line.extend(filter_ids)\n",
    "            out_file.write(' '.join(new_line) + '\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_out/re_item2photos.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        item = line.strip().split(' ')[0]\n",
    "        photos = line.strip().split(' ')[1:]\n",
    "        if len(photos) == 0:\n",
    "            print(f'{item} has no photo')\n",
    "        for photo in photos:\n",
    "            if photo in error_ids:\n",
    "                print(f'{item} has error photo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åä»äº¤äº’æ•°æ®ä¸­åˆ é™¤ è¿‡æ»¤æŸæ¯å›¾åƒå æ— å‰©ä½™å›¾åƒçš„ item å¯¹åº”çš„äº¤äº’è®°å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/filter_yelp_interactions.txt', 'w') as out_file:\n",
    "    with open('yelp_out/re_yelp_interactions.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            item_id = line.strip().split(' ')[1]\n",
    "            if item_id in error_items:\n",
    "                print(f'delete error item {item_id}')\n",
    "                continue\n",
    "            out_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¿”å› Step6 é‡æ–°åˆ’åˆ†æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "è®­ç»ƒé›†å¤§å°: 1429113\n",
      "éªŒè¯é›†å¤§å°: 204159\n",
      "æµ‹è¯•é›†å¤§å°: 408318\n",
      "è®­ç»ƒé›†é‡å¤è¡Œæ•°: 0\n",
      "éªŒè¯é›†é‡å¤è¡Œæ•°: 0\n",
      "æµ‹è¯•é›†é‡å¤è¡Œæ•°: 0\n",
      "è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´é‡å¤è¡Œæ•°: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('yelp_out/filter_yelp_interactions.txt', delimiter=' ', header=None)\n",
    "\n",
    "# æ£€æŸ¥é‡å¤è¡Œ\n",
    "print(f\"åŸå§‹æ•°æ®é‡å¤è¡Œæ•°: {data.duplicated().sum()}\")\n",
    "\n",
    "# åˆ’åˆ†ä¸´æ—¶é›†å’Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸º8:2\n",
    "temp_data, test_data = train_test_split(data, test_size=0.2, random_state=114514)\n",
    "# å°†ä¸´æ—¶é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæ¯”ä¾‹ä¸º7:1\n",
    "train_data, val_data = train_test_split(temp_data, test_size=0.125, random_state=114514)\n",
    "\n",
    "os.makedirs('yelp_ds', exist_ok=True)\n",
    "temp_data.to_csv('yelp_ds/merge_train_data.txt', index=False, header=False, sep=' ')\n",
    "train_data.to_csv('yelp_ds/train_data.txt', index=False, header=False, sep=' ')\n",
    "val_data.to_csv('yelp_ds/val_data.txt', index=False, header=False, sep=' ')\n",
    "test_data.to_csv('yelp_ds/test_data.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "# æŸ¥çœ‹åˆ’åˆ†åçš„æ•°æ®é›†å¤§å°\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_data)}\")\n",
    "print(f\"éªŒè¯é›†å¤§å°: {len(val_data)}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(test_data)}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤é¡¹\n",
    "print(f\"è®­ç»ƒé›†é‡å¤è¡Œæ•°: {train_data.duplicated().sum()}\")\n",
    "print(f\"éªŒè¯é›†é‡å¤è¡Œæ•°: {val_data.duplicated().sum()}\")\n",
    "print(f\"æµ‹è¯•é›†é‡å¤è¡Œæ•°: {test_data.duplicated().sum()}\")\n",
    "\n",
    "# æ£€æŸ¥äº¤å‰é‡å¤é¡¹\n",
    "duplicates_in_all = train_data.merge(val_data, how='inner').merge(test_data, how='inner')\n",
    "print(f\"è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´é‡å¤è¡Œæ•°: {duplicates_in_all.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "## ç”Ÿæˆ RAG æ•°æ®\n",
    "\n",
    "ä» business æ•°æ®ä¸­æå–ï¼Œä»¥æœ€ç»ˆäº¤äº’æ•°æ®ä¸ºè¿‡æ»¤æ¡ä»¶ï¼Œé¢„è®¡å°±æ˜¯ä¸‹é¢è¿™æ ·\n",
    "```\n",
    "business_id categories\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total items: 33183\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "yelp_dataset_path = '/nvme0n1p2/yelp_dataset/'\n",
    "\n",
    "items = set()\n",
    "item_categories = {}\n",
    "with open('yelp_out/filter_yelp_interactions.txt', 'r') as ui_file:\n",
    "    for line in ui_file: items.add(line.split(' ')[1].strip())\n",
    "    print(f'total items: {len(items)}')\n",
    "    with open(yelp_dataset_path + 'yelp_academic_dataset_business.json', 'r') as f:\n",
    "        for line in f:\n",
    "            json_data = json.loads(line)\n",
    "            business_id = json_data['business_id']\n",
    "            categories = json_data['categories']\n",
    "            if business_id in items: item_categories[business_id] = categories\n",
    "\n",
    "# ä¿å­˜item_categoriesåˆ°æ–‡ä»¶\n",
    "with open('yelp_out/yelp_text.json', 'w', encoding='utf-8') as text_file:\n",
    "    json.dump(item_categories, text_file, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33183\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/yelp_text.json', 'r', encoding='utf-8') as text_file:\n",
    "    data = json.load(text_file)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ LLM å¢å¼ºäº¤äº’æ•°æ®\n",
    "\n",
    "### RAGå¢å¼º(deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from pprint import pprint\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"RAG/yelp_rag.json\",\n",
    "    jq_schema=\"{item_id: .business_id, categories: .categories}\",\n",
    "    json_lines=True,\n",
    "    text_content=False\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 1}, page_content='{\"item_id\": \"MTSW4McQd7CbVtyjqoe9mw\", \"categories\": \"Restaurants, Food, Bubble Tea, Coffee & Tea, Bakeries\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 2}, page_content='{\"item_id\": \"bBDDEgkFA1Otx9Lfe7BZUQ\", \"categories\": \"Ice Cream & Frozen Yogurt, Fast Food, Burgers, Restaurants, Food\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 3}, page_content='{\"item_id\": \"eEOYSgkmpB90uNA7lDOMRA\", \"categories\": \"Vietnamese, Food, Restaurants, Food Trucks\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 4}, page_content='{\"item_id\": \"il_Ro8jwPlHresjw9EGmBg\", \"categories\": \"American (Traditional), Restaurants, Diners, Breakfast & Brunch\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 5}, page_content='{\"item_id\": \"0bPLkL0QhhPO5kt1_EXmNQ\", \"categories\": \"Food, Delis, Italian, Bakeries, Restaurants\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 6}, page_content='{\"item_id\": \"MUTTqe8uqyMdBl186RmNeA\", \"categories\": \"Sushi Bars, Restaurants, Japanese\"}')\n",
      "Document(metadata={'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json', 'seq_num': 7}, page_content='{\"item_id\": \"ROeacJQwBeh05Rqg7F6TCg\", \"categories\": \"Korean, Restaurants\"}')\n"
     ]
    }
   ],
   "source": [
    "for idx, d in enumerate(data):\n",
    "    pprint(d)\n",
    "    if idx > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œçš„ Document è¶³å¤Ÿå°äº†ï¼Œæ‰€ä»¥æ— éœ€ split to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# å®ä¾‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "ollama_emb = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"qwen2\", num_gpu=2, show_progress=True)\n",
    "\n",
    "# ä½¿ç”¨æ–‡æ¡£å—åˆ›å»ºå‘é‡æ•°æ®åº“å¹¶æŒä¹…åŒ–\n",
    "persist_directory = \"RAG\"\n",
    "# If a persist_directory is specified, the collection will be persisted there. Otherwise, the data will be ephemeral in-memory.\n",
    "vector_db = Chroma.from_documents(documents=data, \n",
    "                                     embedding=ollama_emb, \n",
    "                                     persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å®ä¾‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "ollama_emb = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"qwen2\", num_gpu=2, show_progress=True)\n",
    "\n",
    "# ä½¿ç”¨æ–‡æ¡£å—åˆ›å»ºå‘é‡æ•°æ®åº“å¹¶æŒä¹…åŒ–\n",
    "persist_directory = \"RAG\"\n",
    "# If a persist_directory is specified, the collection will be persisted there. Otherwise, the data will be ephemeral in-memory.\n",
    "vector_db = Chroma.from_documents(documents=data, \n",
    "                                     embedding=ollama_emb, \n",
    "                                     persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "\n",
    "template = \"\"\"User had interacted with the following businesses:\n",
    "{item_list}\n",
    "\n",
    "The categories of these businesses are in {context}. Please output the business_id only from the following candidate, but not user history.\n",
    "{candidates}\n",
    "\n",
    "Output format:\n",
    "business_ids separated by ','. Nothing else. Please do not output other thing else, do not give reasoning.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2\")\n",
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"item_list\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "        \"candidates\": RunnablePassthrough()\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1051810/2163359439.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  retriever.get_relevant_documents('e4Vwtrqf-wpJfwesgvdgxQ')\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'seq_num': 32384, 'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json'}, page_content='{\"item_id\": \"QKtbbF5-qny5h90Qs3erXw\", \"categories\": \"Indian, Restaurants\"}'),\n",
       " Document(metadata={'seq_num': 14523, 'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json'}, page_content='{\"item_id\": \"Qz6-OxFp9PhGwMZG5geqpw\", \"categories\": \"Italian, American (New), Restaurants\"}'),\n",
       " Document(metadata={'seq_num': 1677, 'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json'}, page_content='{\"item_id\": \"QlI4_BHwxb5UplGwd4vE0w\", \"categories\": \"Mexican, Restaurants\"}'),\n",
       " Document(metadata={'seq_num': 22024, 'source': '/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json'}, page_content='{\"item_id\": \"Q46KberieM6ziVYME6CHEQ\", \"categories\": \"Mexican, Restaurants\"}')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents('e4Vwtrqf-wpJfwesgvdgxQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = []\n",
    "candidates = []\n",
    "with open('yelp_out/filter_yelp_interactions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split(' ')[0].strip() == 'bcjbaE6dDog4jkNY91ncLQ':\n",
    "            item_list.append(line.split(' ')[1].strip())\n",
    "        else:  # éäº¤äº’é¡¹\n",
    "            if len(candidates) <= 5:\n",
    "                candidates.append(line.split(' ')[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='User had interacted with the following businesses:\\n{\\'item_list\\': [\\'e4Vwtrqf-wpJfwesgvdgxQ\\', \\'gUyfJlJRxu1fHuZ4dpBheQ\\', \\'5AOkxsg6UJQ_CoJTMBDUmQ\\', \\'xkYOPbA8AL4jcQIN3xveoQ\\', \\'yJ2ZRXx01eF40eRQFqIBeQ\\', \\'ZyOqGKdr5JetY4jgD_UoGw\\', \\'D73evJ9PZKxO3E5TaThe3w\\', \\'ew5TyXOlyCpCRptye1LdxA\\', \\'q4aAaxdN4wmUZoC6sKEwsw\\', \\'4WdDY97x4GdMYtyk1KQMnw\\', \\'pym7c6ZFEtmoH16xN2ApBg\\', \\'mhrW9O0O5hXGXGnEYBVoag\\', \\'fIBMKVl-dyb3KyM11UBJPQ\\', \\'7gtWQMLOEwCxh1I5j6uB4g\\', \\'qt_E6txwQ1h62wyv8701UQ\\', \\'UjFLIhKTOiFcQiziOA9rgA\\', \\'URxxeb2R60AH81IcuxJAvQ\\', \\'HIomEsnJRxw0861yD87Qgw\\', \\'f9H3wpzWG_apxoumWB-Dvg\\', \\'al3Ri6TEqa2rBzjHsn0T_g\\', \\'mRpk0A4u0hnF0lNe1h4hGg\\', \\'dul6XjaCh1GgA-YHpuChUg\\', \\'5OpNE-GEP1unD89k61XbVQ\\', \\'c-Drp2IuAXSqjvyzvOPBzQ\\', \\'N1we1YLrBxPOoenxJwzdOA\\', \\'Z4PF4EtM12L7nwOHZHFJNA\\', \\'oFbwMxqaCJfIzAEmwaXD3Q\\', \\'uJITgt5t7j-KpDChsXPV5w\\', \\'SjgRHmQ_ClUlECE2JkY8ng\\', \\'Hj_-qd7KyQPRqTWzWgsFag\\', \\'hLr6cRTANzEll1hGlmbgHA\\', \\'nHsoeVL1dXs9ZNjmdrlPuA\\', \\'ecI3FBTM0f99Fnml3kNKfg\\', \\'8kh6Z3c8UHQKmsy0_TbOnA\\', \\'RQpOPNHJReRnrsCD-2qEoA\\', \\'jKTWcdyXPw_cGUp9fKqapQ\\', \\'Se9CEgJEVxcWax1fStWuQA\\', \\'R46XVcmUzy8qeerHyZQtEg\\', \\'dN7AMKUhwTa4Kk0bdMRPgA\\', \\'BFihjoRdU-jmdbvIEqEsxQ\\', \\'3S-u4euLhybQzOuaTAZOpg\\', \\'SkjUwG0FerzrxnIV8N56CA\\', \\'5HZPNcMR5dHQ1OyOb-RDgw\\', \\'Q9GU2OvZObDVyA00ZJkFaA\\', \\'cTSczU-9-cYUEM2DlNJcQw\\', \\'qjP2XXjtLdlZ20SISqtAAA\\', \\'vvOzblHBA2HHsCb7CMSDbQ\\', \\'ncI768qIjMFnMwYMppB4tw\\', \\'2DTkzhmMpv5fIPKheePClA\\', \\'4vak1jxwM6dQ-pNQQ5U8Vw\\', \\'RKfpN_TqD3wa58kgvnR1lQ\\', \\'8UPv1p9GW-BiZtQqUt8nOA\\', \\'ZfzTw5exIOalHsGlK99y-w\\', \\'dju1isgEvDd74tLTDkk5DA\\', \\'dGeXdSMah56gEHwZNaRQKA\\', \\'7H1b6TZ-LNxyGx1cv9suJQ\\', \\'StqG4cdKhTHmGyS7PSimdA\\', \\'if57kE6_VfR1nI1X93oHEA\\', \\'hJTwBhYBTkiHaDMml_v_sw\\', \\'TUTQeLjq1UbkR5r8mOvMqw\\', \\'PKZwdGTapRvFsBYh0zQXpw\\', \\'2x4atI8B9Z0g61bgEOO2Uw\\', \\'WXgV2lOUgas7DzTLeDau-w\\', \\'F3b3-mmClvVPUT0WvK_guA\\', \\'JjUNJCyGQlCxMwOJ9OLdiA\\', \\'EmJF-xSIOaEEOWcK8UOBqg\\', \\'4nrZ42MbdzstpNZaB_Fmew\\', \\'Xar9BJ3iIC9bOQjhXvMYqw\\', \\'HUcjSm1Lxdn8zSu-hQ7-sw\\', \\'8odXL99ki7fi2YPgvAd14w\\', \\'qm17m5Qp7nOQ3meEESLyGw\\', \\'R6P3KSFafpKz6bEtHsWz1A\\'], \\'candidates\\': [\\'RZtGWDLCAtuipwaZ-UfjmQ\\', \\'otQS34_MymijPTdNBoBdCw\\', \\'rBdG_23USc7DletfZ11xGA\\', \\'eFvzHawVJofxSnD7TgbZtg\\', \\'nRKndeZLQ3eDL10UMwS2rQ\\', \\'ut6fi2W2YaipNOqvi7e0jw\\']}\\n\\nThe categories of these businesses are in [Document(metadata={\\'seq_num\\': 6297, \\'source\\': \\'/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json\\'}, page_content=\\'{\"item_id\": \"WvPIOVr2dcUXcTWoloFmTw\", \"categories\": \"Food, Specialty Food, Gluten-Free, Vegan, Pizza, Restaurants\"}\\'), Document(metadata={\\'seq_num\\': 20357, \\'source\\': \\'/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json\\'}, page_content=\\'{\"item_id\": \"cSaoPFiZUzaTgnxM91KgXg\", \"categories\": \"Food, Restaurants, Vegan, Food Delivery Services\"}\\'), Document(metadata={\\'seq_num\\': 29629, \\'source\\': \\'/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json\\'}, page_content=\\'{\"item_id\": \"lRj_1nDUCdXS3LVSrg9vfQ\", \"categories\": \"Specialty Food, Ethnic Food, Grocery, Japanese, Food, Restaurants\"}\\'), Document(metadata={\\'seq_num\\': 9933, \\'source\\': \\'/home/yzh/code/SELFRec/mk_dataset/RAG/yelp_rag.json\\'}, page_content=\\'{\"item_id\": \"wc2MyaDzu8Wq_4DQVMrjSA\", \"categories\": \"Health Markets, Organic Stores, Grocery, Food, Specialty Food\"}\\')]. Based on user\\'s history, please output the business_id only from the following candidate, but not user history.\\n{\\'item_list\\': [\\'e4Vwtrqf-wpJfwesgvdgxQ\\', \\'gUyfJlJRxu1fHuZ4dpBheQ\\', \\'5AOkxsg6UJQ_CoJTMBDUmQ\\', \\'xkYOPbA8AL4jcQIN3xveoQ\\', \\'yJ2ZRXx01eF40eRQFqIBeQ\\', \\'ZyOqGKdr5JetY4jgD_UoGw\\', \\'D73evJ9PZKxO3E5TaThe3w\\', \\'ew5TyXOlyCpCRptye1LdxA\\', \\'q4aAaxdN4wmUZoC6sKEwsw\\', \\'4WdDY97x4GdMYtyk1KQMnw\\', \\'pym7c6ZFEtmoH16xN2ApBg\\', \\'mhrW9O0O5hXGXGnEYBVoag\\', \\'fIBMKVl-dyb3KyM11UBJPQ\\', \\'7gtWQMLOEwCxh1I5j6uB4g\\', \\'qt_E6txwQ1h62wyv8701UQ\\', \\'UjFLIhKTOiFcQiziOA9rgA\\', \\'URxxeb2R60AH81IcuxJAvQ\\', \\'HIomEsnJRxw0861yD87Qgw\\', \\'f9H3wpzWG_apxoumWB-Dvg\\', \\'al3Ri6TEqa2rBzjHsn0T_g\\', \\'mRpk0A4u0hnF0lNe1h4hGg\\', \\'dul6XjaCh1GgA-YHpuChUg\\', \\'5OpNE-GEP1unD89k61XbVQ\\', \\'c-Drp2IuAXSqjvyzvOPBzQ\\', \\'N1we1YLrBxPOoenxJwzdOA\\', \\'Z4PF4EtM12L7nwOHZHFJNA\\', \\'oFbwMxqaCJfIzAEmwaXD3Q\\', \\'uJITgt5t7j-KpDChsXPV5w\\', \\'SjgRHmQ_ClUlECE2JkY8ng\\', \\'Hj_-qd7KyQPRqTWzWgsFag\\', \\'hLr6cRTANzEll1hGlmbgHA\\', \\'nHsoeVL1dXs9ZNjmdrlPuA\\', \\'ecI3FBTM0f99Fnml3kNKfg\\', \\'8kh6Z3c8UHQKmsy0_TbOnA\\', \\'RQpOPNHJReRnrsCD-2qEoA\\', \\'jKTWcdyXPw_cGUp9fKqapQ\\', \\'Se9CEgJEVxcWax1fStWuQA\\', \\'R46XVcmUzy8qeerHyZQtEg\\', \\'dN7AMKUhwTa4Kk0bdMRPgA\\', \\'BFihjoRdU-jmdbvIEqEsxQ\\', \\'3S-u4euLhybQzOuaTAZOpg\\', \\'SkjUwG0FerzrxnIV8N56CA\\', \\'5HZPNcMR5dHQ1OyOb-RDgw\\', \\'Q9GU2OvZObDVyA00ZJkFaA\\', \\'cTSczU-9-cYUEM2DlNJcQw\\', \\'qjP2XXjtLdlZ20SISqtAAA\\', \\'vvOzblHBA2HHsCb7CMSDbQ\\', \\'ncI768qIjMFnMwYMppB4tw\\', \\'2DTkzhmMpv5fIPKheePClA\\', \\'4vak1jxwM6dQ-pNQQ5U8Vw\\', \\'RKfpN_TqD3wa58kgvnR1lQ\\', \\'8UPv1p9GW-BiZtQqUt8nOA\\', \\'ZfzTw5exIOalHsGlK99y-w\\', \\'dju1isgEvDd74tLTDkk5DA\\', \\'dGeXdSMah56gEHwZNaRQKA\\', \\'7H1b6TZ-LNxyGx1cv9suJQ\\', \\'StqG4cdKhTHmGyS7PSimdA\\', \\'if57kE6_VfR1nI1X93oHEA\\', \\'hJTwBhYBTkiHaDMml_v_sw\\', \\'TUTQeLjq1UbkR5r8mOvMqw\\', \\'PKZwdGTapRvFsBYh0zQXpw\\', \\'2x4atI8B9Z0g61bgEOO2Uw\\', \\'WXgV2lOUgas7DzTLeDau-w\\', \\'F3b3-mmClvVPUT0WvK_guA\\', \\'JjUNJCyGQlCxMwOJ9OLdiA\\', \\'EmJF-xSIOaEEOWcK8UOBqg\\', \\'4nrZ42MbdzstpNZaB_Fmew\\', \\'Xar9BJ3iIC9bOQjhXvMYqw\\', \\'HUcjSm1Lxdn8zSu-hQ7-sw\\', \\'8odXL99ki7fi2YPgvAd14w\\', \\'qm17m5Qp7nOQ3meEESLyGw\\', \\'R6P3KSFafpKz6bEtHsWz1A\\'], \\'candidates\\': [\\'RZtGWDLCAtuipwaZ-UfjmQ\\', \\'otQS34_MymijPTdNBoBdCw\\', \\'rBdG_23USc7DletfZ11xGA\\', \\'eFvzHawVJofxSnD7TgbZtg\\', \\'nRKndeZLQ3eDL10UMwS2rQ\\', \\'ut6fi2W2YaipNOqvi7e0jw\\']}\\n\\nOutput format:\\nbusiness_ids separated by \\',\\'. Nothing else. Please do not output other thing else, do not give reasoning.\\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_chain = setup_and_retrieval | prompt\n",
    "output = test_chain.invoke({\n",
    "    \"item_list\": item_list,\n",
    "    \"candidates\": candidates\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.68s/it]\n"
     ]
    }
   ],
   "source": [
    "chain = setup_and_retrieval | prompt | model | StrOutputParser()\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"item_list\": item_list,\n",
    "    \"candidates\": candidates\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tq4dHsAaxAXSqjvyzvOPBzQ, N1we1YLrBxPOoenxJwzdOA, Z4PF4EtM12L7nwOHZHFJNA, oFbwMxqaCJfIzAEmwaXD3Q, uJITgt5t7j-KpDChsXPV5w, SjgRHmQ_ClUlECE2JkY8ng, Hj_-qd7KyQPRqTWzWgsFag, hLr6cRTANzEll1hGlmbgHA, nHsoeVL1dXs9ZNjmdrlPuA, ecI3FBTM0f99Fnml3kNKfg, 8kh6Z3c8UHQKmsy0_TbOnA, RQpOPNHJReRnrsCD-2qEoA, jKTWcdyXPw_cGUp9fKqapQ, Se9CEgJEVxcWax1fStWuQA, R46XVcmUzy8qeerHyZQtEg, dN7AMKUhwTa4Kk0bdMRPgA, BFihjoRdU-jmdbvIEqEsxQ, 3S-u4euLhybQzOuaTAZOpg, SkjUwG0FerzrxnIV8N56CA, 5HZPNcMR5dHQ1OyOb-RDgw, Q9GU2OvZObDVyA00ZJkFaA, cTSczU-9-cYUEM2DlNJcQw, qjP2XXjtLdlZ20SISqtAAA, vvOzblHBA2HHsCb7CMSDbQ, ncI768qIjMFnMwYMppB4tw, 2DTkzhmMpv5fIPKheePClA, 4vak1jxwM6dQ-pNQQ5U8Vw, RKfpN_TqD3wa58kgvnR1lQ, 8UPv1p9GW-BiZtQqUt8nOA, ZfzTw5exIOalHsGlK99y-w, dju1isgEvDd74tLTDkk5DA, dGeXdSMah56gEHwZNaRQKA, 7H1b6TZ-LNxyGx1cv9suJQ, StqG4cdKhTHmGyS7PSimdA, if57kE6_VfR1nI1X93oHEA, hJTwBhYBTkiHaDMml_v_sw, TUTQeLjq1UbkR5r8mOvMqw, PKZwdGTapRvFsBYh0zQXpw, 2x4atI8B9Z0g61bgEOO2Uw, WXgV2lOUgas7DzTLeDau-w, F3b3-mmClvVPUT0WvK_guA, JjUNJCyGQlCxMwOJ9OLdiA, EmJF-xSIOaEEOWcK8UOBqg, 4nrZ42MbdzstpNZaB_Fmew, Xar9BJ3iIC9bOQjhXvMYqw, HUcjSm1Lxdn8zSu-hQ7-sw, 8odXL99ki7fi2YPgvAd14w, qm17m5Qp7nOQ3meEESLyGw, R6P3KSFafpKz6bEtHsWz1A\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¹ˆçœ‹ RAG æ•ˆæœå¹¶ä¸å¥½å•Šã€‚ã€‚ã€‚è€Œä¸”æ–‡æœ¬å†…å®¹å¤ªçŸ­äº†ï¼Œæ•ˆç‡è¿œä¸å¦‚ç›´æ¥æ‹¼æ¥ promptï¼Œè¿˜æµªè´¹æˆ‘å‡ ä¸ªå°æ—¶åš embeddingã€‚ã€‚ã€‚ç„¯\n",
    "\n",
    "ä¸è¿‡ LangChain çš„è¾…åŠ©åŠŸèƒ½è¿˜æ˜¯èƒ½ç”¨ï¼Œæ¯”å¦‚æ¨¡æ¿ä¹‹ç±»çš„ï¼Œæ¯”æ‰‹æ“è¦å¼º\n",
    "\n",
    "### prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_rag = {}\n",
    "with open('RAG/yelp_rag.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        yelp_rag[data['business_id']] = data['categories']        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('MTSW4McQd7CbVtyjqoe9mw', 'Restaurants, Food, Bubble Tea, Coffee & Tea, Bakeries')\n",
      "1 ('bBDDEgkFA1Otx9Lfe7BZUQ', 'Ice Cream & Frozen Yogurt, Fast Food, Burgers, Restaurants, Food')\n",
      "2 ('eEOYSgkmpB90uNA7lDOMRA', 'Vietnamese, Food, Restaurants, Food Trucks')\n",
      "3 ('il_Ro8jwPlHresjw9EGmBg', 'American (Traditional), Restaurants, Diners, Breakfast & Brunch')\n",
      "4 ('0bPLkL0QhhPO5kt1_EXmNQ', 'Food, Delis, Italian, Bakeries, Restaurants')\n"
     ]
    }
   ],
   "source": [
    "for idx, (k, v) in enumerate(yelp_rag.items()):\n",
    "    print(idx, (k, v))\n",
    "    if idx > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import  StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = Ollama(\n",
    "    #base_url=\"http://172.16.110.34:45665\",  # node09\n",
    "    base_url=\"http://localhost:11434\",  # whr\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    num_gpu=2,\n",
    "    # It is recommended to set this value to the number of physical CPU cores\n",
    "    num_thread=48,\n",
    "    # system prompt (overrides what is defined in the Modelfile)\n",
    "    system='You are a recommendation system and required to recommend user with businesses based on user history that each business with id, categories.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"User had interacted with the [history] businesses. Output user's favorite and least favorite business_ids from the [candidates], not from the user [history]. Each line is \"business_id: categories\". Output format are two business_ids separated by ','. The first is favorite and the second is least favorite. No any other things. No reasoning.\n",
    "[history]\n",
    "{item_list}\n",
    "\n",
    "[candidates]\n",
    "{candidates}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = RunnablePassthrough() | prompt | ollama_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = \"\"\"Answer the following question directly without reasoning:\n",
    "{question}\n",
    "\"\"\"\n",
    "test_prompt = ChatPromptTemplate.from_template(test_template)\n",
    "test_chain = RunnablePassthrough() | test_prompt | ollama_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”Ÿæˆæ¨¡æ¿æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4Vwtrqf-wpJfwesgvdgxQ: Sandwiches, Beer, Wine & Spirits, Bars, Food, Restaurants, American (Traditional), Nightlife\n",
      "RZtGWDLCAtuipwaZ-UfjmQ: Pizza, Restaurants, Italian, Salad\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "item_list = []\n",
    "candidates = []\n",
    "\n",
    "# ç”Ÿæˆidæ•°æ®\n",
    "with open('yelp_out/filter_yelp_interactions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split(' ')[0].strip() == 'bcjbaE6dDog4jkNY91ncLQ':\n",
    "            # ä»¥è¿™ä¸ªuserä¸ºä¾‹ï¼Œæ‰¾å‡ºæ‰€æœ‰äº¤äº’çš„items\n",
    "            item_list.append(line.split(' ')[1].strip())\n",
    "        else:  # éäº¤äº’é¡¹ï¼Œå–å‰20ä¸ªè¯•è¯•\n",
    "            if len(candidates) < 3 :\n",
    "                candidates.append(line.split(' ')[1].strip())\n",
    "\n",
    "# æ·»åŠ categoriesæ•°æ®\n",
    "categories = {}\n",
    "with open('yelp_out/yelp_rag.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        categories[data['business_id']] = data['categories']\n",
    "\n",
    "item_list = [f'{item}: {categories[item]}' for item in item_list]\n",
    "filter_item = item_list[:3]\n",
    "candidates = [f'{item}: {categories[item]}' for item in candidates]\n",
    "\n",
    "print(item_list[0])\n",
    "print(candidates[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item_list åº”è¯¥æ˜¯ä¸ªå¤šè¡Œå­—ç¬¦ä¸²ï¼Œåˆ†åˆ«ä¸º id å’Œ categoriesã€‚candidates åŒç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.11 is bigger than 9.9.\n"
     ]
    }
   ],
   "source": [
    "t_output = test_chain.invoke({\n",
    "    \"question\": 'Which number is bigger between 9.9 and 9.11?',\n",
    "})\n",
    "print(t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: User had interacted with the [history] businesses. Output user's favorite and least favorite business_ids from the [candidates], not from the user [history]. Each line is \"business_id: categories\". Output format are two business_ids separated by ','. The first is favorite and the second is least favorite. No any other things. No reasoning.\n",
      "[history]\n",
      "e4Vwtrqf-wpJfwesgvdgxQ: Sandwiches, Beer, Wine & Spirits, Bars, Food, Restaurants, American (Traditional), Nightlife\n",
      "gUyfJlJRxu1fHuZ4dpBheQ: Mexican, Restaurants, Latin American\n",
      "5AOkxsg6UJQ_CoJTMBDUmQ: American (New), Restaurants, Cajun/Creole\n",
      "\n",
      "[candidates]\n",
      "RZtGWDLCAtuipwaZ-UfjmQ: Pizza, Restaurants, Italian, Salad\n",
      "otQS34_MymijPTdNBoBdCw: Restaurants, Tacos, Mexican, Hot Dogs, Breakfast & Brunch, Steakhouses\n",
      "rBdG_23USc7DletfZ11xGA: Wine Bars, Bars, Nightlife, American (New), Mediterranean, Restaurants\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_item_list = \"\\n\".join(filter_item)\n",
    "formatted_candidates = \"\\n\".join(candidates)\n",
    "prompt_chain = RunnablePassthrough() | prompt\n",
    "out_prompt = prompt_chain.invoke({\n",
    "    \"item_list\": formatted_item_list,\n",
    "    \"candidates\": formatted_candidates\n",
    "})\n",
    "print(out_prompt.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RZtGWDLCAtuipwaZ-UfjmQ, otQS34_MymijPTdNBoBdCw\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\n",
    "    \"item_list\": formatted_item_list,\n",
    "    \"candidates\": formatted_candidates\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®å¢å¼º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "item_list = []\n",
    "candidates = []\n",
    "\n",
    "# ç”Ÿæˆidæ•°æ®\n",
    "with open('mk_dataset/yelp_ds_final/train_data.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split(' ')[0].strip() == 'bcjbaE6dDog4jkNY91ncLQ':\n",
    "            # ä»¥è¿™ä¸ªuserä¸ºä¾‹ï¼Œæ‰¾å‡ºæ‰€æœ‰äº¤äº’çš„items\n",
    "            item_list.append(line.split(' ')[1].strip())\n",
    "        else:  # éäº¤äº’é¡¹ï¼Œå–å‰20ä¸ªè¯•è¯•\n",
    "            if len(candidates) < 20 :\n",
    "                candidates.append(line.split(' ')[1].strip())\n",
    "\n",
    "# æ·»åŠ categoriesæ•°æ®\n",
    "categories = {}\n",
    "with open('yelp_out/yelp_rag.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        categories[data['business_id']] = data['categories']\n",
    "item_list = [f'{item}: {categories[item]}' for item in item_list]\n",
    "candidates = [f'{item}: {categories[item]}' for item in candidates]\n",
    "\n",
    "print(item_list[0])\n",
    "print(candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t ç»Ÿè®¡é‡: 30.792014356777727, p-value: 0.0010530218790085494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# æ¨¡å‹ A å’Œæ¨¡å‹ B çš„ Recall@20 å€¼\n",
    "recall_A = np.array([0.1480,0.1481,0.1482])\n",
    "recall_B = np.array([0.14649,0.14650,0.14651])\n",
    "\n",
    "# è®¡ç®—æ¯æ¬¡å®éªŒçš„å·®å¼‚\n",
    "differences = recall_A - recall_B\n",
    "\n",
    "# è¿›è¡Œé…å¯¹ t æ£€éªŒ\n",
    "t_stat, p_value = stats.ttest_rel(recall_A, recall_B)\n",
    "\n",
    "print(f\"t ç»Ÿè®¡é‡: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®å¢å¼º2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = Ollama(\n",
    "    #base_url=\"http://172.16.110.34:45665\",  # node09\n",
    "    base_url=\"http://localhost:11434\",  # whr\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    num_gpu=2,\n",
    "    # It is recommended to set this value to the number of physical CPU cores\n",
    "    num_thread=48,\n",
    "    # system prompt (overrides what is defined in the Modelfile)\n",
    "    system='You are a recommendation system and required to recommend user with businesses based on user history that each business with id, categories.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯•ä¸€ä¸‹èƒ½ä¸èƒ½ä»æ–‡ä»¶è¯»å–æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User had interacted with the following businesses. Each line is \"business_id: categories\". Please summarize this user preference categories with historical information. Give answers directly. Do not output any other things. Do not give reasoning.\n",
      "\n",
      "[history]\n",
      "{history}\n",
      "\n",
      "[output format]\n",
      "categorie1, categorie2, ...\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"User had interacted with the following businesses. Each line is \"business_id: categories\". Please summarize this user preference with historical information. Give answers directly. Do not output any other things. Do not give reasoning.\n",
    "\n",
    "[history]\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "template_file = '/home/yzh/code/SELFRec/conf/aug_prompt.txt'\n",
    "with open(template_file, 'r') as f:\n",
    "    template = f.read()\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_out/yelp_text.json', 'r') as text_file:\n",
    "    yelp_text = json.load(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = RunnablePassthrough() | prompt | ollama_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e4Vwtrqf-wpJfwesgvdgxQ: Sandwiches, Beer, Wine & Spirits, Bars, Food, Restaurants, American (Traditional), Nightlife', 'gUyfJlJRxu1fHuZ4dpBheQ: Mexican, Restaurants, Latin American', '5AOkxsg6UJQ_CoJTMBDUmQ: American (New), Restaurants, Cajun/Creole']\n"
     ]
    }
   ],
   "source": [
    "item_list = []\n",
    "\n",
    "# ç”Ÿæˆidæ•°æ®\n",
    "with open('yelp_out/filter_yelp_interactions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split(' ')[0].strip() == 'bcjbaE6dDog4jkNY91ncLQ':\n",
    "            # ä»¥è¿™ä¸ªuserä¸ºä¾‹ï¼Œæ‰¾å‡ºæ‰€æœ‰äº¤äº’çš„items\n",
    "            item_list.append(line.split(' ')[1].strip())\n",
    "\n",
    "item_list = [f'{item}: {yelp_text[item]}' for item in item_list]\n",
    "filter_item = item_list[:3]\n",
    "\n",
    "print(filter_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: User had interacted with the following businesses. Each line is \"business_id: categories\". Please summarize this user preference categories with historical information. Give answers directly. Do not output any other things. Do not give reasoning.\n",
      "\n",
      "[history]\n",
      "e4Vwtrqf-wpJfwesgvdgxQ: Sandwiches, Beer, Wine & Spirits, Bars, Food, Restaurants, American (Traditional), Nightlife\n",
      "gUyfJlJRxu1fHuZ4dpBheQ: Mexican, Restaurants, Latin American\n",
      "5AOkxsg6UJQ_CoJTMBDUmQ: American (New), Restaurants, Cajun/Creole\n",
      "\n",
      "[output format]\n",
      "categorie1, categorie2, ...\n"
     ]
    }
   ],
   "source": [
    "form_item_list = '\\n'.join(filter_item)\n",
    "prompt_chain = RunnablePassthrough() | prompt\n",
    "out_prompt = prompt_chain.invoke({\n",
    "    \"history\": form_item_list,\n",
    "})\n",
    "print(out_prompt.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American (Traditional), Mexican, American (New)\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\n",
    "    \"history\": form_item_list,\n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆå¹¶æ–‡ä»¶\n",
    "\n",
    "ç”±äºå­˜åœ¨è¯·æ±‚ä¸­æ–­çš„æƒ…å†µï¼Œæ‰€ä»¥åˆ†æ‰¹æ¬¡äº§ç”Ÿäº†è‹¥å¹²ä¸ªç”Ÿæˆæ–‡ä»¶ï¼Œä¸ºä¸é—æ¼ï¼Œé€šè¿‡idè¿›è¡Œæ§åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116507\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('yelp_out/yelp_user_history.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2id = {}\n",
    "id2user = {}\n",
    "for i, user in enumerate(data):\n",
    "    user2id[user] = i\n",
    "    id2user[i] = user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116449"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user2id['_fCu_7tmTX-DevfSyoyqsg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116506"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user2id['V9fW3-fJ-sEMz_ewPpzXXg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oe3JA8llbDetMWxSPjJHVA'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2user[3339]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”Ÿæˆå®Œæ¯•ï¼Œç»Ÿè®¡æ•°æ®é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116507"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8649+30000+77800+58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-YOWyjJ0bOdSN0LfDSLC4Q': ''}\n"
     ]
    }
   ],
   "source": [
    "with open('/nvme0n1p2/yelp_out/yelp_user_preference-20240929_0832.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data: dict = json.loads(line)\n",
    "        if list(data.keys())[0] == '-YOWyjJ0bOdSN0LfDSLC4Q':\n",
    "            print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆå¹¶ç”Ÿæˆæ–‡ä»¶(æ–‡ä»¶ä¸å¤§ï¼Œå°±ä¸å†™æµäº†)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    '/nvme0n1p2/yelp_out/yelp_user_preference-20240930_1006.json',  # 0-8648\n",
    "    '/nvme0n1p2/yelp_out/yelp_user_preference-20240928_2011.json',  # 8649-38648\n",
    "    '/nvme0n1p2/yelp_out/yelp_user_preference-20240929_0832.json',  # 38649-116448\n",
    "    '/nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json'   # 116449-116506\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8649 /nvme0n1p2/yelp_out/yelp_user_preference-20240930_1006.json\n",
      "30000 /nvme0n1p2/yelp_out/yelp_user_preference-20240928_2011.json\n",
      "77800 /nvme0n1p2/yelp_out/yelp_user_preference-20240929_0832.json\n",
      "58 /nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wc -l /nvme0n1p2/yelp_out/yelp_user_preference-20240930_1006.json\n",
    "wc -l /nvme0n1p2/yelp_out/yelp_user_preference-20240928_2011.json\n",
    "wc -l /nvme0n1p2/yelp_out/yelp_user_preference-20240929_0832.json\n",
    "wc -l /nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116507"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8649+30000+77800+58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_fCu_7tmTX-DevfSyoyqsg\": \"restaurants, bars\"}\n",
      "{\"V9fW3-fJ-sEMz_ewPpzXXg\": \"Music Venues, Bars\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head /nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json -n 1\n",
    "tail /nvme0n1p2/yelp_out/yelp_user_preference-20240930_0837.json -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json line -> json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "user_preferences: dict[str, str] = {}  # user->categories\n",
    "with open('yelp_user_preferences.v2.json', 'w', encoding='utf-8') as fout:\n",
    "    for path in file_paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data: dict[str, str] = json.loads(line)  # user->categories\n",
    "                    for k,v in data.items():  # å…¶å®åªæœ‰ä¸€ç»„\n",
    "                        if k in user_preferences: # åº”è¯¥ä¸ä¼šæœ‰\n",
    "                            print('exist duplicate user: ', k)\n",
    "                        user_preferences[k] = v\n",
    "                except Exception as e:\n",
    "                    with open('error_preferences.txt', 'a', encoding='utf-8') as errors:\n",
    "                        errors.write(f\"{path.split('-')[1]}: {line}\")\n",
    "    \n",
    "    json.dump(user_preferences, fout, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 116507 users, 73 empty fields\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "with open('yelp_user_preferences.v1.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for k, v in data.items():\n",
    "        if v.strip() == \"\":\n",
    "            error += 1\n",
    "    print(f\"total {len(data)} users, {error} empty fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰§è¡Œ `python ollama_aug.py --type specific` åï¼Œæ›´æ–° `yelp_user_preferences.json`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 116507\n",
      "fix: 73\n",
      "after: 116507\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('yelp_user_preferences.v1.json', 'r') as origin:\n",
    "    origin_data: dict[str, str] = json.load(origin)\n",
    "    print(f\"before: {len(origin_data)}\")\n",
    "\n",
    "    fix_path = '/nvme0n1p2/yelp_out/yelp_user_preference-20241007_1708.json'\n",
    "    fix_data: dict[str, str] = {}\n",
    "    with open(fix_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data: dict[str, str] = json.loads(line)\n",
    "            for k,v in data.items():\n",
    "                fix_data[k] = v\n",
    "    print(f\"fix: {len(fix_data)}\")\n",
    "\n",
    "    # æ›´æ–°æ•°æ®\n",
    "    with open('yelp_user_preferences.v2.json', 'w') as output:\n",
    "        for k in fix_data.keys():\n",
    "            if k in origin_data:\n",
    "                origin_data[k] = fix_data[k]\n",
    "            else:\n",
    "                print(f\"{k} is in fix data but not in origin data!\")\n",
    "        json.dump(origin_data, output, ensure_ascii=False)\n",
    "    print(f\"after: {len(origin_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 116507 users, 0 empty fields\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "with open('yelp_user_preferences.v2.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for k, v in data.items():\n",
    "        if v.strip() == \"\":\n",
    "            error += 1\n",
    "    print(f\"total {len(data)} users, {error} empty fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å†æ¬¡æ·»åŠ ï¼Œç›´æ¥æ„å»ºå…¨å±€çš„å§ï¼Œåæ­£å·®ä¸äº†å‡ ä¸ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 116507\n",
      "fix: 508\n",
      "after: 117015\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('yelp_out/yelp_user_preferences.v2.json', 'r') as origin:\n",
    "    origin_data: dict[str, str] = json.load(origin)\n",
    "    print(f\"before: {len(origin_data)}\")\n",
    "\n",
    "    fix_path = 'yelp_out/preferences/yelp_user_preference-20241022_2023.json'\n",
    "    fix_data: dict[str, str] = {}\n",
    "    with open(fix_path, 'r') as f:\n",
    "        data: dict[str, str] = json.load(f)\n",
    "        for k,v in data.items():\n",
    "            fix_data[k] = v\n",
    "    print(f\"fix: {len(fix_data)}\")\n",
    "\n",
    "    # æ›´æ–°æ•°æ®\n",
    "    with open('yelp_user_preferences.v3.json', 'w') as output:\n",
    "        for k, v in fix_data.items():\n",
    "            if k in origin_data:\n",
    "                print('exist')\n",
    "            else:\n",
    "                origin_data[k] = v\n",
    "        json.dump(origin_data, output, ensure_ascii=False)\n",
    "    print(f\"after: {len(origin_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ–‡æœ¬åµŒå…¥ç”Ÿæˆ\n",
    "\n",
    "[stella_en_1.5B_v5](https://huggingface.co/bennegeek/stella_en_1.5B_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "query_prompt_name = \"s2s_query\"\n",
    "model = SentenceTransformer('/home/yzh/code/SELFRec/model/stella_en_1.5B_v5', device=\"cuda:1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”Ÿæˆ user_pref_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "with open('yelp_out/yelp_user_preferences.v3.json', 'r', encoding='utf-8') as file:\n",
    "    user_preferences: dict[str, str] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total text: 116507\n",
      "output embedding shape: torch.Size([116507, 1024])\n"
     ]
    }
   ],
   "source": [
    "user_pre_embs: dict[str, torch.Tensor] = {}\n",
    "user_pre_list: list[str] = []\n",
    "for prefs in user_preferences.values():\n",
    "    user_pre_list.append(prefs)\n",
    "print(f'total text: {len(user_pre_list)}')\n",
    "\n",
    "# æ¨¡å‹ç¼–ç \n",
    "pre_embs = model.encode(user_pre_list, prompt_name=query_prompt_name, device='cuda:1', batch_size=16, convert_to_tensor=True)\n",
    "print(f\"output embedding shape: {pre_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fix user pref: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117015/117015 [00:15<00:00, 7533.11it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pref_embs = load_file(\"yelp_ds/user_pre_embs.safetensors\", device=\"cuda:1\")\n",
    "for user in tqdm(user_preferences, desc='fix user pref'):\n",
    "    if user in pref_embs: continue\n",
    "    emb = model.encode(user_preferences[user], prompt_name=query_prompt_name, device='cuda:1', convert_to_tensor=True)\n",
    "    pref_embs[user] = emb\n",
    "\n",
    "metadata = {\n",
    "    \"type\": \"pt\",\n",
    "    \"user num\": \"117015\",\n",
    "    \"dim\": \"1024\",\n",
    "    \"build by\": \"https://github.com/sun2ot\",\n",
    "    \"time\": \"2024-10-22 20:51\"\n",
    "}\n",
    "save_file(pref_embs, \"user_pre_embs.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117015\n"
     ]
    }
   ],
   "source": [
    "print(len(pref_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116507\n",
      "tensor([-2.0780,  1.0266, -0.3841,  ..., -1.3006, -2.8999,  3.2373],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "loaded = load_file('user_pre_embs.safetensors', device='cuda:1')\n",
    "print(len(loaded))\n",
    "print(loaded['GziM44xJcoR4jJByq10NQA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116507\n",
      "tensor([-2.0780,  1.0266, -0.3841,  ..., -1.3006, -2.8999,  3.2373],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "with safe_open('user_pre_embs.safetensors', framework='pt', device='cuda:1') as f: # type: ignore\n",
    "    print(len(f.keys()))\n",
    "    print(f.get_tensor('GziM44xJcoR4jJByq10NQA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æ³•ç‚®åˆ¶, ç”Ÿæˆ text_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total text: 33183\n",
      "output embedding shape: torch.Size([33183, 1024])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "with open('yelp_out/yelp_text.json', 'r', encoding='utf-8') as text_file:\n",
    "    item_text: dict[str, str] = json.load(text_file)\n",
    "\n",
    "item_text_embs: dict[str, torch.Tensor] = {}\n",
    "item_text_list: list[str] = []\n",
    "for text in item_text.values():\n",
    "    item_text_list.append(text)\n",
    "print(f'total text: {len(item_text_list)}')\n",
    "\n",
    "text_embs = model.encode(item_text_list, device='cuda:1', prompt_name=query_prompt_name, batch_size=32, convert_to_tensor=True)\n",
    "print(f\"output embedding shape: {text_embs.shape}\")\n",
    "\n",
    "for idx, item in enumerate(item_text.keys()):\n",
    "    item_text_embs[item] = text_embs[idx]\n",
    "save_file(item_text_embs, 'item_text_embs.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33183\n",
      "tensor([-0.8660,  2.7190, -0.9495,  ...,  1.4988, -0.4805,  2.8815],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "with safe_open('item_text_embs.safetensors', framework='pt', device='cuda:1') as f: # type: ignore\n",
    "    print(len(f.keys()))\n",
    "    print(f.get_tensor('MTSW4McQd7CbVtyjqoe9mw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¼©å°æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…ˆæ„å»ºäº¤äº’å­—å…¸ï¼Œä¾¿äºåç»­å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "ui_dict = defaultdict(dict)\n",
    "with open('yelp_ds/filter_yelp_interactions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        user, item, ratings = line.strip().split(' ')\n",
    "        ui_dict[user][item] = ratings\n",
    "\n",
    "# æŒä¹…åŒ–\n",
    "with open('yelp_ds/ui_dict.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(ui_dict, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117015\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_ds/ui_dict.json', 'r', encoding='utf-8') as file:\n",
    "    ui_data = json.load(file)\n",
    "print(len(ui_data))\n",
    "del ui_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 117015\n",
      "del not_core_user: 79618\n",
      "users: 37397, items: 32491, it_num: 707178\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open('yelp_ds/ui_dict.json', 'r', encoding='utf-8') as file:\n",
    "    # æœ‰è¯„åˆ†, æµ®ç‚¹å‹, ä»¥å­—ç¬¦ä¸²ä¿å­˜\n",
    "    ui_dict: dict[str, dict[str, str]] = json.load(file)\n",
    "origin = len(ui_dict)\n",
    "print(f\"origin: {origin}\")\n",
    "\n",
    "not_core_user=0\n",
    "for user, item_ratings in list(ui_dict.items()):\n",
    "    if len(item_ratings) < 15:\n",
    "        not_core_user+=1\n",
    "        # å…ˆåˆ é™¤äº¤äº’ä¸è¶³15æ¬¡çš„user\n",
    "        del ui_dict[user]\n",
    "print(f\"del not_core_user: {not_core_user}\")\n",
    "\n",
    "for user, item_ratings in ui_dict.items():\n",
    "    if len(item_ratings) < 10: raise Exception('exit user it < 10')\n",
    "    if len(item_ratings) > 20:\n",
    "        # ä¿ç•™è‡³å¤š20æ¬¡äº¤äº’\n",
    "        select_items = random.sample(list(item_ratings.keys()), 20)\n",
    "        ui_dict[user] = {item: item_ratings[item] for item in select_items}\n",
    "\n",
    "item_set = set()\n",
    "it_num = 0\n",
    "for user, item_ratings in ui_dict.items():\n",
    "    it_num += len(item_ratings)\n",
    "    item_set.update(item_ratings.keys())\n",
    "\n",
    "print(f\"users: {len(ui_dict)}, items: {len(item_set)}, it_num: {it_num}\")\n",
    "\n",
    "\n",
    "# æŒä¹…åŒ–\n",
    "with open('yelp_tiny/ui_dict.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(ui_dict, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37397\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_tiny/ui_dict.json', 'r', encoding='utf-8') as file:\n",
    "    d = json.load(file)\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### åˆ’åˆ†æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_tiny/ui_dict.json', 'r', encoding='utf-8') as file:\n",
    "    ui_dict = json.load(file)\n",
    "\n",
    "train_set = []\n",
    "val_set = []\n",
    "test_set = []\n",
    "not_core = 0\n",
    "\n",
    "for user, item_ragings in ui_dict.items():\n",
    "    items = list(item_ragings.keys())\n",
    "    random.shuffle(items)  # æ‰“ä¹±äº¤äº’æ•°æ®\n",
    "    it_num = len(items)\n",
    "    if it_num < 10: not_core+=1  # ä¸è¶³10ä¸ªä¼šå¯¼è‡´å¯¹åº”æµ‹è¯•é›†ä¸ºç©º\n",
    "    train_num = int(it_num * 0.7)\n",
    "    val_num = int(it_num * 0.1) + train_num\n",
    "\n",
    "    # å°†æ¯ä¸ªç”¨æˆ·çš„äº¤äº’è®°å½•åˆ’åˆ†åˆ°ä¸‰ä¸ªå­é›†\n",
    "    for item in items[:train_num]: train_set.append((user, item, item_ragings[item]))\n",
    "    for item in items[train_num:val_num]: val_set.append((user, item, item_ragings[item]))\n",
    "    for item in items[val_num:]: test_set.append((user, item, item_ragings[item]))\n",
    "print(not_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488814\n",
      "62360\n",
      "156004\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def save_dataset(dataset: list[tuple[str, str, str]], filename: str):\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        for user, item, ratings in tqdm(dataset):\n",
    "            f.write(f\"{user} {item} {ratings}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 488814/488814 [00:00<00:00, 2066120.92it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62360/62360 [00:00<00:00, 1542025.35it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156004/156004 [00:00<00:00, 1704365.84it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dataset(train_set, 'yelp_tiny/train.txt')\n",
    "save_dataset(val_set, 'yelp_tiny/val.txt')\n",
    "save_dataset(test_set, 'yelp_tiny/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 551174/551174 [00:00<00:00, 1696203.46it/s]\n"
     ]
    }
   ],
   "source": [
    "merge_train = []\n",
    "for i in train_set: merge_train.append(i)\n",
    "for j in val_set: merge_train.append(j)\n",
    "print(len(merge_train))\n",
    "save_dataset(merge_train, 'yelp_tiny/merge_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å›¾åƒæ¨¡æ€äºŒæ¬¡å¤„ç†\n",
    "\n",
    "yelp å›¾åƒæ¨¡æ€æ•°æ®éœ€è¦å†å¤„ç†ä¸‹ï¼Œå¤šå›¾ç›´æ¥åˆä¸€ï¼Œé¿å…æ¨¡å‹è¿‡ç¨‹ä¸­å¤„ç†ï¼Œå½±å“ä»£ç å¤ç”¨æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "item images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33183/33183 [00:09<00:00, 3548.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "import torch\n",
    "\n",
    "item2image: dict[str, list[str]] = {}\n",
    "with safe_open('yelp_ds/photo_embs.safetensors', 'pt', device=\"cuda:0\") as image_safetensors: # type: ignore\n",
    "    item_set = set()\n",
    "    with open('yelp_out/re_item2photos.txt', 'r') as map_file:\n",
    "        for line in map_file:\n",
    "            item = line.strip().split(' ')[0]\n",
    "            item_set.add(item)\n",
    "            images = line.strip().split(' ')[1:]\n",
    "            item2image[item] = images\n",
    "\n",
    "    merge_image_tensor: dict[str, torch.Tensor] = {}\n",
    "    for item in tqdm(item_set, desc='item images'):\n",
    "        # è¿™é‡Œæ˜¯å…¨å±€å›¾åƒ, ä¸æ˜¯ tiny\n",
    "        try:\n",
    "            merge_image_tensor[item] = torch.mean(\n",
    "                torch.stack([image_safetensors.get_tensor(image) for image in item2image[item]]), dim=0\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            exit(-1)\n",
    "    \n",
    "    save_file(merge_image_tensor, 'yelp_ds/item_image_emb.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
