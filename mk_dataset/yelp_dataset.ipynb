{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ds_util import head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_dataset_path = '/nvme0n1p2/yelp_dataset/'\n",
    "yelp_photo_path = '/nvme0n1p2/yelp_photos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# äº¤äº’æ•°æ®\n",
    "\n",
    "## Step1ï¼šä» review æ•°æ®é›†ä¸­æå–æ‰€æœ‰ç”¨æˆ·ï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„è¯„è®ºæ•°é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ user_id ä¸ºé›†åˆ\n",
    "user_ids = set()\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_user.json', 'r', encoding='utf-8') as f_users:\n",
    "    for line in f_users:\n",
    "        user_data = json.loads(line)\n",
    "        user_ids.add(user_data['user_id'])\n",
    "\n",
    "# ç»Ÿè®¡æ¯ä¸ª user_id çš„å®é™… reviews æ•°é‡\n",
    "user_review_count = {}\n",
    "\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        user_id = review_data['user_id']\n",
    "        if user_id in user_ids:\n",
    "            if user_id in user_review_count:\n",
    "                user_review_count[user_id] += 1\n",
    "            else:\n",
    "                user_review_count[user_id] = 1\n",
    "\n",
    "# ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "with open('yelp_out/user_review_count.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for user_id, count in user_review_count.items():\n",
    "        output_file.write(f\"{user_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_-eMZ6K5RLWhZyISBhwA 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/user_review_count.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2ï¼šç­›é€‰å‡ºäº¤äº’æ¬¡æ•°ï¼ˆè¯„è®ºæ•°ï¼‰å¤§äºç­‰äº 10 æ¬¡çš„ç”¨æˆ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "items = {}\n",
    "with open('yelp_out/user_review_count.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # count_data -> [user_id, review_count]\n",
    "        count_data = line.strip().split(' ')\n",
    "        if int(count_data[1]) >= 10:\n",
    "            items[count_data[0]] = count_data[1]\n",
    "        if int(count_data[1]) < 1:\n",
    "            print(f'user {count_data[0]} has 0 review!')\n",
    "\n",
    "# ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "with open('yelp_out/core_users.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for user_id, count in items.items():\n",
    "        output_file.write(f\"{user_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_-eMZ6K5RLWhZyISBhwA 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/core_users.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3ï¼šä» review æ•°æ®é›†ä¸­æå–æ‰€æœ‰é¡¹ç›®å•†å®¶ï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªå•†å®¶çš„è¯„è®ºæ•°é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ business_id ä¸ºé›†åˆ\n",
    "business_ids = set()\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_business.json', 'r', encoding='utf-8') as f_item:\n",
    "    for line in f_item:\n",
    "        item_data = json.loads(line)\n",
    "        business_ids.add(item_data['business_id'])\n",
    "\n",
    "# ç»Ÿè®¡æ¯ä¸ª business_id çš„å®é™… reviews æ•°é‡\n",
    "item_review_count = {}\n",
    "\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        business_id = review_data['business_id']\n",
    "        if business_id in business_ids:\n",
    "            if business_id in item_review_count:\n",
    "                item_review_count[business_id] += 1\n",
    "            else:\n",
    "                item_review_count[business_id] = 1\n",
    "\n",
    "# ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "with open('yelp_out/item_review_count.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for business_id, count in item_review_count.items():\n",
    "        output_file.write(f\"{business_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQfwVwDr-v0ZS3_CbbE5Xw 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/item_review_count.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4ï¼šç­›é€‰å‡ºäº¤äº’æ¬¡æ•°å¤§äºç­‰äº 10 æ¬¡ä¸”å­˜åœ¨ photos æ•°æ®çš„å•†å®¶\n",
    "\n",
    "ç†è®ºä¸Šè¯´ï¼Œ10-core settings åªéœ€è¦äº¤äº’æ¬¡æ•°é™åˆ¶å°±å¯ä»¥äº†ã€‚ä½†æœ¬æ–‡ç”±äºéœ€è¦å›¾ç‰‡æ¨¡æ€æ•°æ®ï¼Œè€Œå®é™…å¤„ç†è¿‡ç¨‹ä¸­å‘ç°å¦‚æœä¸æ·»åŠ â€œæœ‰å›¾ç‰‡â€çš„é™åˆ¶æ¡ä»¶ï¼Œç”Ÿæˆçš„æ•°æ®é›†ä¸­ï¼Œå°†æœ‰çº¦ 67% çš„ business æ²¡æœ‰å¯¹åº”çš„ photos æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "items = {}\n",
    "business_with_photos = set()\n",
    "\n",
    "try:\n",
    "    with open(yelp_photo_path + 'photos.json', 'r') as photo_list:\n",
    "        for line in photo_list:\n",
    "            data = json.loads(line)\n",
    "            business_id = data['business_id']\n",
    "            business_with_photos.add(business_id)\n",
    "\n",
    "    with open('yelp_out/item_review_count.txt', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # count_data -> [business_id, review_count]\n",
    "            count_data = line.strip().split(' ')\n",
    "            if int(count_data[1]) >= 10 and count_data[0] in business_with_photos:\n",
    "                items[count_data[0]] = count_data[1]\n",
    "            if int(count_data[1]) < 1:\n",
    "                print(f'business {count_data[0]} has 0 review!')\n",
    "\n",
    "    # ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶\n",
    "    with open('yelp_out/re_core_items.txt', 'w', encoding='utf-8') as output_file:\n",
    "        for business_id, count in items.items():\n",
    "            output_file.write(f\"{business_id} {count}\\n\")\n",
    "        \n",
    "        print(\"generate complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'errors: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQfwVwDr-v0ZS3_CbbE5Xw 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/core_items.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5ï¼šäº¤å‰è¿‡æ»¤-ä» review æ•°æ®é›†ä¸­æå–å‡º 10-core äº¤äº’è®°å½•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼Œyelp æ•°æ®é›†è‡ªå¸¦æ¢è¡Œç¬¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        if '\\n' in line: \n",
    "            print('yes')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤å¤–ï¼Œç”±äºå­˜åœ¨ä¸€ä¸ªç”¨æˆ·å¯¹åŒä¸€å•†å®¶çš„å¤šæ¡è¯„è®ºè®°å½•ï¼Œå› æ­¤æ„é€ è¿‡æ»¤ review æ•°æ®é›†æ—¶ï¼Œéœ€è€ƒè™‘ä»¥ä¸‹å‡ æ–¹é¢ï¼š\n",
    "1. ç”¨æˆ·å’Œé¡¹ç›®éƒ½æ˜¯æ ¸å¿ƒç”¨æˆ·/é¡¹ç›®\n",
    "2. (user_id ,business_id) ä¸é‡å¤\n",
    "3. é’ˆå¯¹é‡å¤çš„æƒ…å†µï¼Œå–è¯„åˆ†æœ€é«˜é¡¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter reviews done\n"
     ]
    }
   ],
   "source": [
    "core_users = set()\n",
    "items = set()\n",
    "# ä¿è¯ä¸€ä¸ªç”¨æˆ·å¯¹ä¸€ä¸ªé¡¹ç›®åªæœ‰ä¸€æ¡äº¤äº’è®°å½•\n",
    "user_item_pairs = {}\n",
    "\n",
    "with open('yelp_out/core_users.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        user_id = line.strip().split(' ')[0]\n",
    "        core_users.add(user_id)\n",
    "\n",
    "with open('yelp_out/re_core_items.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        business_id = line.strip().split(' ')[0]\n",
    "        items.add(business_id)\n",
    "\n",
    "filter_reviews = []  # -> [user_id, business_id, stars, review_id]\n",
    "index = 0\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        user_id = review_data['user_id']\n",
    "        business_id = review_data['business_id']\n",
    "\n",
    "        if user_id in core_users and business_id in items:\n",
    "            if (user_id, business_id) not in user_item_pairs:\n",
    "                # ç”¨å“ˆå¸Œè¡¨åŒæ—¶è®°å½•è¯„åˆ†å’Œç´¢å¼•ï¼Œä»¥ä¾¿åç»­æ“ä½œ\n",
    "                user_item_pairs[(user_id, business_id)] = [review_data['stars'], index]\n",
    "                # filter_reviews.append([user_id, business_id, review_data['stars'], review_data['review_id']])\n",
    "                filter_reviews.append([user_id, business_id, review_data['stars']])\n",
    "                index += 1\n",
    "            else:\n",
    "                if review_data['stars'] > user_item_pairs[(user_id, business_id)][0]:\n",
    "                    pair_index = user_item_pairs[(user_id, business_id)][1]\n",
    "                    # æ›´æ–°æ•°æ®\n",
    "                    # filter_reviews[pair_index] = [user_id, business_id, review_data['stars'], review_data['review_id']]\n",
    "                    filter_reviews[pair_index] = [user_id, business_id, review_data['stars']]\n",
    "\n",
    "# æŒä¹…åŒ–è¿‡æ»¤åçš„è¯„è®ºæ•°æ®é›†\n",
    "with open('yelp_out/re_yelp_interactions.txt', 'w', encoding='utf-8') as out_file:\n",
    "    for record in filter_reviews:\n",
    "        out_file.write(' '.join(map(str, record)) + '\\n')\n",
    "    print('Filter reviews done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcjbaE6dDog4jkNY91ncLQ e4Vwtrqf-wpJfwesgvdgxQ 4.0\n",
      "\n",
      "smOvOajNG0lS4Pq7d8g4JQ RZtGWDLCAtuipwaZ-UfjmQ 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/re_yelp_interactions.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6ï¼šåˆ’åˆ†æ•°æ®é›†\n",
    "\n",
    "å‚è€ƒ [SELFRec issue 54](https://github.com/Coder-Yu/SELFRec/issues/54)ï¼Œé¢†åŸŸå†…åœ¨å¾—åˆ°æœ€ç»ˆç»“æœæ—¶ï¼Œä¼¼ä¹ä¼šæŠŠè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆå¹¶ï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥æŒ‰ 7:3 åˆ’åˆ†è®­ç»ƒé›†è·Ÿæµ‹è¯•é›†å¥½äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "è®­ç»ƒé›†å¤§å°: 1429167\n",
      "æµ‹è¯•é›†å¤§å°: 612501\n",
      "è®­ç»ƒæ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "æµ‹è¯•æ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„é‡å¤è¡Œ:\n",
      "Empty DataFrame\n",
      "Columns: [0, 1, 2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('yelp_out/re_yelp_interactions.txt', delimiter=' ', header=None)\n",
    "\n",
    "# æ£€æŸ¥é‡å¤è¡Œ\n",
    "print(f\"åŸå§‹æ•°æ®é‡å¤è¡Œæ•°: {data.duplicated().sum()}\")\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸º7:3\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=114514)\n",
    "\n",
    "train_data.to_csv('yelp_ds_re/train_data.txt', index=False, header=False, sep=' ')\n",
    "test_data.to_csv('yelp_ds_re/test_data.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "# æŸ¥çœ‹åˆ’åˆ†åçš„æ•°æ®é›†å¤§å°\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_data)}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(test_data)}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤é¡¹\n",
    "print(f\"è®­ç»ƒæ•°æ®é‡å¤è¡Œæ•°: {train_data.duplicated().sum()}\")\n",
    "print(f\"æµ‹è¯•æ•°æ®é‡å¤è¡Œæ•°: {test_data.duplicated().sum()}\")\n",
    "duplicates_in_train = train_data.merge(test_data, how='inner')\n",
    "print(f'è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„é‡å¤è¡Œ:\\n{duplicates_in_train}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å›¾åƒæ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. æ ¹æ®ç”Ÿæˆçš„ interactions æ•°æ®ï¼Œå¯¹ç¬¬äºŒåˆ—å»ºç«‹é›†åˆï¼Œå³æ•°æ®é›†ä¸­æ‰€æœ‰ business_id\n",
    "2. å°† business_id ä»£å…¥ `photos/` æ£€ç´¢å‡ºæ‰€æœ‰ photo_id å¹¶ä¿å­˜æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å›¾åƒè®°å½•æ•°æ®å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"photo_id\": \"zsvj7vloL4L5jhYyPIuVwg\",\n",
      "  \"business_id\": \"Nk-SJhPlDBkAZvfsADtccA\",\n",
      "  \"caption\": \"Nice rock artwork everywhere and craploads of taps.\",\n",
      "  \"label\": \"inside\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "head(yelp_photo_path + 'photos.json', jsonf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å•†å®¶-å›¾ç‰‡æ˜ å°„è®°å½•\n",
    "\n",
    "ä»æœ€ç»ˆçš„**äº¤äº’æ•°æ®é›†**ä¸­ï¼Œæå–å‡ºå•†å®¶-å›¾ç‰‡(ä¸€å¯¹å¤š)çš„æ˜ å°„å…³ç³»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "photo_ids = set()\n",
    "items = set()\n",
    "item2photos = {}\n",
    "\n",
    "try:\n",
    "    with open('yelp_out/re_yelp_interactions.txt', 'r') as interact_file:\n",
    "        for line in interact_file:\n",
    "            items.add(line.split(' ')[1])\n",
    "            item2photos[line.split(' ')[1]] = []\n",
    "    \n",
    "    # item: photo_id1, photo_id2, ...\n",
    "    with open('yelp_out/item2photos.txt', 'w', encoding='utf-8') as out_file:\n",
    "        with open(yelp_photo_path + 'photos.json', 'r', encoding='utf-8') as photos:\n",
    "            for line in photos:\n",
    "                json_data = json.loads(line)\n",
    "                photo_id = json_data['photo_id']\n",
    "                business_id = json_data['business_id']\n",
    "\n",
    "                # å­˜åœ¨ photo_id é‡å¤çš„æƒ…å†µ\n",
    "                if photo_id not in photo_ids and business_id in items:\n",
    "                    item2photos[business_id].append(photo_id)\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        for item in item2photos:\n",
    "            out_file.write(item + ' ' + ' '.join(item2photos[item]) + '\\n')\n",
    "        print('generate complete')\n",
    "except Exception as e:\n",
    "    print(f'error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 business without any photo\n",
      "occupy 0.0%\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/item2photos.txt', 'r') as f:\n",
    "    no_photos_item = 0\n",
    "    total_item = 0\n",
    "\n",
    "    for line in f:\n",
    "        total_item += 1\n",
    "        parts = line.split(' ')\n",
    "        business_id = parts[0]\n",
    "        photo_ids = parts[1:]\n",
    "\n",
    "        if photo_ids[0] == '\\n':\n",
    "            no_photos_item += 1\n",
    "\n",
    "    print(f'{no_photos_item} business without any photo')\n",
    "    print(f'occupy {no_photos_item/total_item*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ [CLIP-ViT](https://huggingface.co/openai/clip-vit-base-patch32) å¯¹å›¾åƒè¿›è¡Œç¼–ç \n",
    "\n",
    "- [tutorial 1](https://medium.com/@highsunday0630/image-embedding-1-clip%E6%A8%A1%E5%9E%8B%E6%8F%90%E5%8F%96-image-embedding-%E4%B8%A6%E4%BB%A5-tensorboard-%E8%A6%96%E8%A6%BA%E5%8C%96%E6%95%88%E6%9E%9C-dc281370d7d8)\n",
    "- [tutorial 2](https://blog.csdn.net/qq_37756660/article/details/135979873)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from safetensors.torch import save_file\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "import os\n",
    "import tqdm\n",
    "import logging\n",
    "import torch\n",
    "from torch.nn.parallel import DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/yzh/code/SELFRec/model/clip-vit-base-patch32\"\n",
    "# å›¾ç‰‡å­˜å‚¨è·¯å¾„\n",
    "directory = '/nvme0n1p2/yelp_photos/photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total record photos: 190902\n"
     ]
    }
   ],
   "source": [
    "photos = set()  # æ‰€æœ‰éœ€è¦å¤„ç†çš„å›¾ç‰‡\n",
    "with open('yelp_out/item2photos.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        for photo_id in line.strip().split(' ')[1:]:\n",
    "            photos.add(photo_id)\n",
    "print('total record photos:', len(photos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yelp æ•°æ®é›†ä¸­çš„ photo æ•°æ®å­˜åœ¨è„æ•°æ®(æ— æ³•è¯†åˆ«ã€æŸå)\n",
    "\n",
    "> æˆ‘æ˜¯æ²¡æƒ³åˆ°å…¬å¼€æ•°æ®é›†è¿è¿™ä¸ªéƒ½ä¸å¤„ç†å¥½ğŸ˜…\n",
    "\n",
    "å…ˆå†™ä¸ªæ—¥å¿—çœ‹çœ‹æœ‰å¤šå°‘è„æ•°æ®å§ï¼ˆæ‰‹å¯åˆ«æŠ–ï¼Œä¸‹é¢çš„æ—¥å¿—æ¨¡å—ä»£ç åˆ‡è®°æ‰§è¡Œä¸€æ¬¡ï¼Œè¿™ä¸æ¯”åˆ«çš„ï¼Œå¤šæ‰§è¡Œå‡ æ¬¡ä¼šå¤šå‡ºå‡ ä¸ªloggerï¼Œåˆ°æ—¶å€™ä½ çš„æ—¥å¿—é‡Œéƒ½æ˜¯é‡å¤çš„ä¿¡æ¯ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "photo_logger = logging.getLogger('error_photo')\n",
    "photo_logger.setLevel(logging.INFO)\n",
    "sh = logging.FileHandler('../log/error_photo.log')\n",
    "sh.setLevel(logging.WARNING)\n",
    "formatter = logging.Formatter('%(name)s - %(message)s')\n",
    "sh.setFormatter(formatter)\n",
    "photo_logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "200098it [27:28, 121.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 190902 photos, processed: 190797, errors: 105, len of photo_embs: 190797\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "# model = CLIPModel.from_pretrained(model_path).to(f'cuda:{device_ids[0]}')\n",
    "model = CLIPModel.from_pretrained(model_path).to('cuda')\n",
    "processor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "# å®šä¹‰æ‰¹é‡å¤§å°\n",
    "batch_size = 64\n",
    "\n",
    "# åˆå§‹åŒ–è®¡æ•°å™¨\n",
    "photo_num = 0\n",
    "processed_num = 0\n",
    "error_num = 0\n",
    "photo_embs = {}\n",
    "\n",
    "# è¯»å–ç›®å½•ä¸­çš„å›¾ç‰‡\n",
    "with os.scandir(directory) as entries:\n",
    "    images = []  # å¾…å¤„ç†å›¾ç‰‡åˆ—è¡¨\n",
    "    photo_ids = []  # å›¾ç‰‡IDåˆ—è¡¨\n",
    "\n",
    "    # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    for entry in tqdm.tqdm(entries):\n",
    "        photo_id = entry.name.replace('.jpg', '')\n",
    "        if photo_id not in photos:\n",
    "            continue\n",
    "        photo_num += 1\n",
    "        try:\n",
    "            images.append(Image.open(entry.path))\n",
    "            photo_ids.append(photo_id)\n",
    "        except Exception as e:\n",
    "            photo_logger.warning(f'{entry.name} error: {e}')\n",
    "            error_num += 1\n",
    "            continue\n",
    "\n",
    "        # å½“è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼Œå¤„ç†è¿™æ‰¹å›¾ç‰‡\n",
    "        if len(images) == batch_size:\n",
    "            inputs = processor(images=images, return_tensors='pt')\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                # ä½¿ç”¨get_image_featuresè·å–å›¾åƒç‰¹å¾\n",
    "                outputs = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "            for idx, pid in enumerate(photo_ids):\n",
    "                photo_embs[pid] = outputs[idx]\n",
    "            processed_num += batch_size\n",
    "            \n",
    "            # æ¸…ç©ºåˆ—è¡¨\n",
    "            images = []\n",
    "            photo_ids = []\n",
    "\n",
    "    # å¤„ç†å‰©ä½™çš„å›¾ç‰‡\n",
    "    if images:\n",
    "        inputs = processor(images=images, return_tensors='pt')\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            # ä½¿ç”¨get_image_featuresè·å–å›¾åƒç‰¹å¾\n",
    "            outputs = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "        for index, pid in enumerate(photo_ids):\n",
    "            photo_embs[pid] = outputs[index]\n",
    "        processed_num += len(images)\n",
    "\n",
    "print(f'total {photo_num} photos, processed: {processed_num}, errors: {error_num}, len of photo_embs: {len(photo_embs)}')\n",
    "# ä¿å­˜ä¸ºæ–‡ä»¶\n",
    "save_file(photo_embs, 'photo_embs.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "100.0% photos are in the dataset\n"
     ]
    }
   ],
   "source": [
    "with safe_open('photo_embs.safetensors', framework='pt') as f:\n",
    "    total_num = len(f.keys())\n",
    "    photo_num = 0\n",
    "    for k in f.keys():\n",
    "        if k in photos: photo_num += 1\n",
    "    for k in f.keys():\n",
    "        print(f.get_tensor(k).shape)\n",
    "        break\n",
    "    print(f'{photo_num/total_num*100}% photos are in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŒ‰ç…§åŸæ¥çš„æ–¹æ¡ˆï¼Œæœ‰105å¼ æŸåå›¾åƒï¼Œç°åœ¨ä»æ˜ å°„æ•°æ® `item2photots.txt` ä¸­åˆ é™¤æ— æ•ˆå›¾ç‰‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ids = set()  # é”™è¯¯å›¾åƒidé›†åˆ\n",
    "with open('../log/error_photo.log', 'r') as f:\n",
    "    for line in f:\n",
    "        photo_id = line.strip().split(' - ')[1].split(' ')[0].replace('.jpg', '')\n",
    "        if photo_id not in error_ids:\n",
    "            error_ids.add(photo_id)\n",
    "        else:\n",
    "            print(f'exist duplicate error photo: {photo_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWs1xspH1d-NCIWmXM40RQ remain no photoğŸ˜­\n",
      "qJKMyChtpyqPvf7HNfvq4A remain no photoğŸ˜­\n",
      "vpkCctZV4_q7iUkmtdZkzQ remain no photoğŸ˜­\n",
      "dBa7aJXV50TZEtInwdbvfg remain no photoğŸ˜­\n",
      "LIh33t2G-y0C1H3o41xJSQ remain no photoğŸ˜­\n",
      "djn6PlsuFw_Z_gRA55QDcg remain no photoğŸ˜­\n",
      "a1Bd6IhR_Bsthhff9VGLoA remain no photoğŸ˜­\n"
     ]
    }
   ],
   "source": [
    "error_items = set()\n",
    "with open('yelp_out/re_item2photos.txt', 'w') as out_file:\n",
    "    with open('yelp_out/item2photos.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            new_line = []\n",
    "            item_id = line.strip().split(' ')[0]\n",
    "            photo_ids = line.strip().split(' ')[1:]\n",
    "            filter_ids = [id for id in photo_ids if id not in error_ids]\n",
    "            if len(filter_ids) == 0:\n",
    "                print(f'{item_id} remain no photoğŸ˜­')\n",
    "                error_items.add(item_id)\n",
    "                continue\n",
    "            new_line.append(item_id)\n",
    "            new_line.extend(filter_ids)\n",
    "            out_file.write(' '.join(new_line) + '\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_out/re_item2photos.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        item = line.strip().split(' ')[0]\n",
    "        photos = line.strip().split(' ')[1:]\n",
    "        if len(photos) == 0:\n",
    "            print(f'{item} has no photo')\n",
    "        for photo in photos:\n",
    "            if photo in error_ids:\n",
    "                print(f'{item} has error photo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åä»äº¤äº’æ•°æ®ä¸­åˆ é™¤æ— å›¾åƒçš„itemå¯¹åº”çš„äº¤äº’è®°å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/filter_yelp_interactions.txt', 'w') as out_file:\n",
    "    with open('yelp_out/re_yelp_interactions.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            item_id = line.strip().split(' ')[1]\n",
    "            if item_id in error_items:\n",
    "                print(f'delete error item {item_id}')\n",
    "                continue\n",
    "            out_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿”å› Step6 é‡æ–°åˆ’åˆ†æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "è®­ç»ƒé›†å¤§å°: 1429113\n",
      "æµ‹è¯•é›†å¤§å°: 612477\n",
      "è®­ç»ƒæ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "æµ‹è¯•æ•°æ®é‡å¤è¡Œæ•°: 0\n",
      "è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„é‡å¤è¡Œ:\n",
      "Empty DataFrame\n",
      "Columns: [0, 1, 2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('yelp_out/filter_yelp_interactions.txt', delimiter=' ', header=None)\n",
    "\n",
    "# æ£€æŸ¥é‡å¤è¡Œ\n",
    "print(f\"åŸå§‹æ•°æ®é‡å¤è¡Œæ•°: {data.duplicated().sum()}\")\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸º7:3\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=114514)\n",
    "\n",
    "os.makedirs('yelp_ds_final', exist_ok=True)\n",
    "train_data.to_csv('yelp_ds_final/train_data.txt', index=False, header=False, sep=' ')\n",
    "test_data.to_csv('yelp_ds_final/test_data.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "# æŸ¥çœ‹åˆ’åˆ†åçš„æ•°æ®é›†å¤§å°\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_data)}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(test_data)}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤é¡¹\n",
    "print(f\"è®­ç»ƒæ•°æ®é‡å¤è¡Œæ•°: {train_data.duplicated().sum()}\")\n",
    "print(f\"æµ‹è¯•æ•°æ®é‡å¤è¡Œæ•°: {test_data.duplicated().sum()}\")\n",
    "duplicates_in_train = train_data.merge(test_data, how='inner')\n",
    "print(f'è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„é‡å¤è¡Œ:\\n{duplicates_in_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
