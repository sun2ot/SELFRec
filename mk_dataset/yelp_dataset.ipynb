{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ds_util import head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_dataset_path = '/nvme0n1p2/yelp_dataset/'\n",
    "yelp_photo_path = '/nvme0n1p2/yelp_photos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交互数据\n",
    "\n",
    "## Step1：从 review 数据集中提取所有用户，并统计每个用户的评论数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "# 加载 user_id 为集合\n",
    "user_ids = set()\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_user.json', 'r', encoding='utf-8') as f_users:\n",
    "    for line in f_users:\n",
    "        user_data = json.loads(line)\n",
    "        user_ids.add(user_data['user_id'])\n",
    "\n",
    "# 统计每个 user_id 的实际 reviews 数量\n",
    "user_review_count = {}\n",
    "\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        user_id = review_data['user_id']\n",
    "        if user_id in user_ids:\n",
    "            if user_id in user_review_count:\n",
    "                user_review_count[user_id] += 1\n",
    "            else:\n",
    "                user_review_count[user_id] = 1\n",
    "\n",
    "# 保存为本地文件\n",
    "with open('yelp_out/user_review_count.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for user_id, count in user_review_count.items():\n",
    "        output_file.write(f\"{user_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_-eMZ6K5RLWhZyISBhwA 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/user_review_count.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2：筛选出交互次数（评论数）大于等于 10 次的用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "items = {}\n",
    "with open('yelp_out/user_review_count.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # count_data -> [user_id, review_count]\n",
    "        count_data = line.strip().split(' ')\n",
    "        if int(count_data[1]) >= 10:\n",
    "            items[count_data[0]] = count_data[1]\n",
    "        if int(count_data[1]) < 1:\n",
    "            print(f'user {count_data[0]} has 0 review!')\n",
    "\n",
    "# 保存为本地文件\n",
    "with open('yelp_out/core_users.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for user_id, count in items.items():\n",
    "        output_file.write(f\"{user_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mh_-eMZ6K5RLWhZyISBhwA 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/core_users.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3：从 review 数据集中提取所有项目商家，并统计每个商家的评论数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "# 加载 business_id 为集合\n",
    "business_ids = set()\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_business.json', 'r', encoding='utf-8') as f_item:\n",
    "    for line in f_item:\n",
    "        item_data = json.loads(line)\n",
    "        business_ids.add(item_data['business_id'])\n",
    "\n",
    "# 统计每个 business_id 的实际 reviews 数量\n",
    "item_review_count = {}\n",
    "\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        business_id = review_data['business_id']\n",
    "        if business_id in business_ids:\n",
    "            if business_id in item_review_count:\n",
    "                item_review_count[business_id] += 1\n",
    "            else:\n",
    "                item_review_count[business_id] = 1\n",
    "\n",
    "# 保存为本地文件\n",
    "with open('yelp_out/item_review_count.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for business_id, count in item_review_count.items():\n",
    "        output_file.write(f\"{business_id} {count}\\n\")\n",
    "    \n",
    "    print(\"generate complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQfwVwDr-v0ZS3_CbbE5Xw 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/item_review_count.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4：筛选出交互次数大于等于 10 次且存在 photos 数据的商家\n",
    "\n",
    "理论上说，10-core settings 只需要交互次数限制就可以了。但本文由于需要图片模态数据，而实际处理过程中发现如果不添加“有图片”的限制条件，生成的数据集中，将有约 67% 的 business 没有对应的 photos 数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "items = {}\n",
    "business_with_photos = set()\n",
    "\n",
    "try:\n",
    "    with open(yelp_photo_path + 'photos.json', 'r') as photo_list:\n",
    "        for line in photo_list:\n",
    "            data = json.loads(line)\n",
    "            business_id = data['business_id']\n",
    "            business_with_photos.add(business_id)\n",
    "\n",
    "    with open('yelp_out/item_review_count.txt', 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # count_data -> [business_id, review_count]\n",
    "            count_data = line.strip().split(' ')\n",
    "            if int(count_data[1]) >= 10 and count_data[0] in business_with_photos:\n",
    "                items[count_data[0]] = count_data[1]\n",
    "            if int(count_data[1]) < 1:\n",
    "                print(f'business {count_data[0]} has 0 review!')\n",
    "\n",
    "    # 保存为本地文件\n",
    "    with open('yelp_out/re_core_items.txt', 'w', encoding='utf-8') as output_file:\n",
    "        for business_id, count in items.items():\n",
    "            output_file.write(f\"{business_id} {count}\\n\")\n",
    "        \n",
    "        print(\"generate complete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'errors: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XQfwVwDr-v0ZS3_CbbE5Xw 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/core_items.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5：交叉过滤-从 review 数据集中提取出 10-core 交互记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，yelp 数据集自带换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        if '\\n' in line: \n",
    "            print('yes')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，由于存在一个用户对同一商家的多条评论记录，因此构造过滤 review 数据集时，需考虑以下几方面：\n",
    "1. 用户和项目都是核心用户/项目\n",
    "2. (user_id ,business_id) 不重复\n",
    "3. 针对重复的情况，取评分最高项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter reviews done\n"
     ]
    }
   ],
   "source": [
    "core_users = set()\n",
    "items = set()\n",
    "# 保证一个用户对一个项目只有一条交互记录\n",
    "user_item_pairs = {}\n",
    "\n",
    "with open('yelp_out/core_users.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        user_id = line.strip().split(' ')[0]\n",
    "        core_users.add(user_id)\n",
    "\n",
    "with open('yelp_out/re_core_items.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        business_id = line.strip().split(' ')[0]\n",
    "        items.add(business_id)\n",
    "\n",
    "filter_reviews = []  # -> [user_id, business_id, stars, review_id]\n",
    "index = 0\n",
    "with open(yelp_dataset_path + 'yelp_academic_dataset_review.json', 'r', encoding='utf-8') as review_file:\n",
    "    for line in review_file:\n",
    "        review_data = json.loads(line)\n",
    "        user_id = review_data['user_id']\n",
    "        business_id = review_data['business_id']\n",
    "\n",
    "        if user_id in core_users and business_id in items:\n",
    "            if (user_id, business_id) not in user_item_pairs:\n",
    "                # 用哈希表同时记录评分和索引，以便后续操作\n",
    "                user_item_pairs[(user_id, business_id)] = [review_data['stars'], index]\n",
    "                # filter_reviews.append([user_id, business_id, review_data['stars'], review_data['review_id']])\n",
    "                filter_reviews.append([user_id, business_id, review_data['stars']])\n",
    "                index += 1\n",
    "            else:\n",
    "                if review_data['stars'] > user_item_pairs[(user_id, business_id)][0]:\n",
    "                    pair_index = user_item_pairs[(user_id, business_id)][1]\n",
    "                    # 更新数据\n",
    "                    # filter_reviews[pair_index] = [user_id, business_id, review_data['stars'], review_data['review_id']]\n",
    "                    filter_reviews[pair_index] = [user_id, business_id, review_data['stars']]\n",
    "\n",
    "# 持久化过滤后的评论数据集\n",
    "with open('yelp_out/re_yelp_interactions.txt', 'w', encoding='utf-8') as out_file:\n",
    "    for record in filter_reviews:\n",
    "        out_file.write(' '.join(map(str, record)) + '\\n')\n",
    "    print('Filter reviews done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcjbaE6dDog4jkNY91ncLQ e4Vwtrqf-wpJfwesgvdgxQ 4.0\n",
      "\n",
      "smOvOajNG0lS4Pq7d8g4JQ RZtGWDLCAtuipwaZ-UfjmQ 4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head('yelp_out/re_yelp_interactions.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6：划分数据集\n",
    "\n",
    "参考 [SELFRec issue 54](https://github.com/Coder-Yu/SELFRec/issues/54)，领域内在得到最终结果时，似乎会把训练集和验证集合并，所以这里直接按 7:3 划分训练集跟测试集好了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据重复行数: 0\n",
      "训练集大小: 1429167\n",
      "测试集大小: 612501\n",
      "训练数据重复行数: 0\n",
      "测试数据重复行数: 0\n",
      "训练集和测试集中的重复行:\n",
      "Empty DataFrame\n",
      "Columns: [0, 1, 2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('yelp_out/re_yelp_interactions.txt', delimiter=' ', header=None)\n",
    "\n",
    "# 检查重复行\n",
    "print(f\"原始数据重复行数: {data.duplicated().sum()}\")\n",
    "\n",
    "# 划分训练集和测试集，比例为7:3\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=114514)\n",
    "\n",
    "train_data.to_csv('yelp_ds_re/train_data.txt', index=False, header=False, sep=' ')\n",
    "test_data.to_csv('yelp_ds_re/test_data.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "# 查看划分后的数据集大小\n",
    "print(f\"训练集大小: {len(train_data)}\")\n",
    "print(f\"测试集大小: {len(test_data)}\")\n",
    "\n",
    "# 检查是否有重复项\n",
    "print(f\"训练数据重复行数: {train_data.duplicated().sum()}\")\n",
    "print(f\"测试数据重复行数: {test_data.duplicated().sum()}\")\n",
    "duplicates_in_train = train_data.merge(test_data, how='inner')\n",
    "print(f'训练集和测试集中的重复行:\\n{duplicates_in_train}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 根据生成的 interactions 数据，对第二列建立集合，即数据集中所有 business_id\n",
    "2. 将 business_id 代入 `photos/` 检索出所有 photo_id 并保存文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像记录数据展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"photo_id\": \"zsvj7vloL4L5jhYyPIuVwg\",\n",
      "  \"business_id\": \"Nk-SJhPlDBkAZvfsADtccA\",\n",
      "  \"caption\": \"Nice rock artwork everywhere and craploads of taps.\",\n",
      "  \"label\": \"inside\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "head(yelp_photo_path + 'photos.json', jsonf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 商家-图片映射记录\n",
    "\n",
    "从最终的**交互数据集**中，提取出商家-图片(一对多)的映射关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "photo_ids = set()\n",
    "items = set()\n",
    "item2photos = {}\n",
    "\n",
    "try:\n",
    "    with open('yelp_out/re_yelp_interactions.txt', 'r') as interact_file:\n",
    "        for line in interact_file:\n",
    "            items.add(line.split(' ')[1])\n",
    "            item2photos[line.split(' ')[1]] = []\n",
    "    \n",
    "    # item: photo_id1, photo_id2, ...\n",
    "    with open('yelp_out/item2photos.txt', 'w', encoding='utf-8') as out_file:\n",
    "        with open(yelp_photo_path + 'photos.json', 'r', encoding='utf-8') as photos:\n",
    "            for line in photos:\n",
    "                json_data = json.loads(line)\n",
    "                photo_id = json_data['photo_id']\n",
    "                business_id = json_data['business_id']\n",
    "\n",
    "                # 存在 photo_id 重复的情况\n",
    "                if photo_id not in photo_ids and business_id in items:\n",
    "                    item2photos[business_id].append(photo_id)\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        for item in item2photos:\n",
    "            out_file.write(item + ' ' + ' '.join(item2photos[item]) + '\\n')\n",
    "        print('generate complete')\n",
    "except Exception as e:\n",
    "    print(f'error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 business without any photo\n",
      "occupy 0.0%\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/item2photos.txt', 'r') as f:\n",
    "    no_photos_item = 0\n",
    "    total_item = 0\n",
    "\n",
    "    for line in f:\n",
    "        total_item += 1\n",
    "        parts = line.split(' ')\n",
    "        business_id = parts[0]\n",
    "        photo_ids = parts[1:]\n",
    "\n",
    "        if photo_ids[0] == '\\n':\n",
    "            no_photos_item += 1\n",
    "\n",
    "    print(f'{no_photos_item} business without any photo')\n",
    "    print(f'occupy {no_photos_item/total_item*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 [CLIP-ViT](https://huggingface.co/openai/clip-vit-base-patch32) 对图像进行编码\n",
    "\n",
    "- [tutorial 1](https://medium.com/@highsunday0630/image-embedding-1-clip%E6%A8%A1%E5%9E%8B%E6%8F%90%E5%8F%96-image-embedding-%E4%B8%A6%E4%BB%A5-tensorboard-%E8%A6%96%E8%A6%BA%E5%8C%96%E6%95%88%E6%9E%9C-dc281370d7d8)\n",
    "- [tutorial 2](https://blog.csdn.net/qq_37756660/article/details/135979873)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from safetensors.torch import save_file\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "import os\n",
    "import tqdm\n",
    "import logging\n",
    "import torch\n",
    "from torch.nn.parallel import DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/yzh/code/SELFRec/model/clip-vit-base-patch32\"\n",
    "# 图片存储路径\n",
    "directory = '/nvme0n1p2/yelp_photos/photos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total record photos: 190902\n"
     ]
    }
   ],
   "source": [
    "photos = set()  # 所有需要处理的图片\n",
    "with open('yelp_out/item2photos.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        for photo_id in line.strip().split(' ')[1:]:\n",
    "            photos.add(photo_id)\n",
    "print('total record photos:', len(photos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yelp 数据集中的 photo 数据存在脏数据(无法识别、损坏)\n",
    "\n",
    "> 我是没想到公开数据集连这个都不处理好😅\n",
    "\n",
    "先写个日志看看有多少脏数据吧（手可别抖，下面的日志模块代码切记执行一次，这不比别的，多执行几次会多出几个logger，到时候你的日志里都是重复的信息）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "photo_logger = logging.getLogger('error_photo')\n",
    "photo_logger.setLevel(logging.INFO)\n",
    "sh = logging.FileHandler('../log/error_photo.log')\n",
    "sh.setLevel(logging.WARNING)\n",
    "formatter = logging.Formatter('%(name)s - %(message)s')\n",
    "sh.setFormatter(formatter)\n",
    "photo_logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/yzh/env/miniforge3/envs/selfrec/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "200098it [27:28, 121.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 190902 photos, processed: 190797, errors: 105, len of photo_embs: 190797\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "# model = CLIPModel.from_pretrained(model_path).to(f'cuda:{device_ids[0]}')\n",
    "model = CLIPModel.from_pretrained(model_path).to('cuda')\n",
    "processor = CLIPProcessor.from_pretrained(model_path)\n",
    "\n",
    "# 定义批量大小\n",
    "batch_size = 64\n",
    "\n",
    "# 初始化计数器\n",
    "photo_num = 0\n",
    "processed_num = 0\n",
    "error_num = 0\n",
    "photo_embs = {}\n",
    "\n",
    "# 读取目录中的图片\n",
    "with os.scandir(directory) as entries:\n",
    "    images = []  # 待处理图片列表\n",
    "    photo_ids = []  # 图片ID列表\n",
    "\n",
    "    # 遍历目录中的所有文件\n",
    "    for entry in tqdm.tqdm(entries):\n",
    "        photo_id = entry.name.replace('.jpg', '')\n",
    "        if photo_id not in photos:\n",
    "            continue\n",
    "        photo_num += 1\n",
    "        try:\n",
    "            images.append(Image.open(entry.path))\n",
    "            photo_ids.append(photo_id)\n",
    "        except Exception as e:\n",
    "            photo_logger.warning(f'{entry.name} error: {e}')\n",
    "            error_num += 1\n",
    "            continue\n",
    "\n",
    "        # 当达到批量大小时，处理这批图片\n",
    "        if len(images) == batch_size:\n",
    "            inputs = processor(images=images, return_tensors='pt')\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                # 使用get_image_features获取图像特征\n",
    "                outputs = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "            for idx, pid in enumerate(photo_ids):\n",
    "                photo_embs[pid] = outputs[idx]\n",
    "            processed_num += batch_size\n",
    "            \n",
    "            # 清空列表\n",
    "            images = []\n",
    "            photo_ids = []\n",
    "\n",
    "    # 处理剩余的图片\n",
    "    if images:\n",
    "        inputs = processor(images=images, return_tensors='pt')\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            # 使用get_image_features获取图像特征\n",
    "            outputs = model.get_image_features(pixel_values=inputs['pixel_values'])\n",
    "        for index, pid in enumerate(photo_ids):\n",
    "            photo_embs[pid] = outputs[index]\n",
    "        processed_num += len(images)\n",
    "\n",
    "print(f'total {photo_num} photos, processed: {processed_num}, errors: {error_num}, len of photo_embs: {len(photo_embs)}')\n",
    "# 保存为文件\n",
    "save_file(photo_embs, 'photo_embs.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "100.0% photos are in the dataset\n"
     ]
    }
   ],
   "source": [
    "with safe_open('photo_embs.safetensors', framework='pt') as f:\n",
    "    total_num = len(f.keys())\n",
    "    photo_num = 0\n",
    "    for k in f.keys():\n",
    "        if k in photos: photo_num += 1\n",
    "    for k in f.keys():\n",
    "        print(f.get_tensor(k).shape)\n",
    "        break\n",
    "    print(f'{photo_num/total_num*100}% photos are in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照原来的方案，有105张损坏图像，现在从映射数据 `item2photots.txt` 中删除无效图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ids = set()  # 错误图像id集合\n",
    "with open('../log/error_photo.log', 'r') as f:\n",
    "    for line in f:\n",
    "        photo_id = line.strip().split(' - ')[1].split(' ')[0].replace('.jpg', '')\n",
    "        if photo_id not in error_ids:\n",
    "            error_ids.add(photo_id)\n",
    "        else:\n",
    "            print(f'exist duplicate error photo: {photo_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWs1xspH1d-NCIWmXM40RQ remain no photo😭\n",
      "qJKMyChtpyqPvf7HNfvq4A remain no photo😭\n",
      "vpkCctZV4_q7iUkmtdZkzQ remain no photo😭\n",
      "dBa7aJXV50TZEtInwdbvfg remain no photo😭\n",
      "LIh33t2G-y0C1H3o41xJSQ remain no photo😭\n",
      "djn6PlsuFw_Z_gRA55QDcg remain no photo😭\n",
      "a1Bd6IhR_Bsthhff9VGLoA remain no photo😭\n"
     ]
    }
   ],
   "source": [
    "error_items = set()\n",
    "with open('yelp_out/re_item2photos.txt', 'w') as out_file:\n",
    "    with open('yelp_out/item2photos.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            new_line = []\n",
    "            item_id = line.strip().split(' ')[0]\n",
    "            photo_ids = line.strip().split(' ')[1:]\n",
    "            filter_ids = [id for id in photo_ids if id not in error_ids]\n",
    "            if len(filter_ids) == 0:\n",
    "                print(f'{item_id} remain no photo😭')\n",
    "                error_items.add(item_id)\n",
    "                continue\n",
    "            new_line.append(item_id)\n",
    "            new_line.extend(filter_ids)\n",
    "            out_file.write(' '.join(new_line) + '\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yelp_out/re_item2photos.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        item = line.strip().split(' ')[0]\n",
    "        photos = line.strip().split(' ')[1:]\n",
    "        if len(photos) == 0:\n",
    "            print(f'{item} has no photo')\n",
    "        for photo in photos:\n",
    "            if photo in error_ids:\n",
    "                print(f'{item} has error photo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后从交互数据中删除无图像的item对应的交互记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item WWs1xspH1d-NCIWmXM40RQ\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item qJKMyChtpyqPvf7HNfvq4A\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item vpkCctZV4_q7iUkmtdZkzQ\n",
      "delete error item dBa7aJXV50TZEtInwdbvfg\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item LIh33t2G-y0C1H3o41xJSQ\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item djn6PlsuFw_Z_gRA55QDcg\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n",
      "delete error item a1Bd6IhR_Bsthhff9VGLoA\n"
     ]
    }
   ],
   "source": [
    "with open('yelp_out/filter_yelp_interactions.txt', 'w') as out_file:\n",
    "    with open('yelp_out/re_yelp_interactions.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            item_id = line.strip().split(' ')[1]\n",
    "            if item_id in error_items:\n",
    "                print(f'delete error item {item_id}')\n",
    "                continue\n",
    "            out_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回 Step6 重新划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据重复行数: 0\n",
      "训练集大小: 1429113\n",
      "测试集大小: 612477\n",
      "训练数据重复行数: 0\n",
      "测试数据重复行数: 0\n",
      "训练集和测试集中的重复行:\n",
      "Empty DataFrame\n",
      "Columns: [0, 1, 2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('yelp_out/filter_yelp_interactions.txt', delimiter=' ', header=None)\n",
    "\n",
    "# 检查重复行\n",
    "print(f\"原始数据重复行数: {data.duplicated().sum()}\")\n",
    "\n",
    "# 划分训练集和测试集，比例为7:3\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=114514)\n",
    "\n",
    "os.makedirs('yelp_ds_final', exist_ok=True)\n",
    "train_data.to_csv('yelp_ds_final/train_data.txt', index=False, header=False, sep=' ')\n",
    "test_data.to_csv('yelp_ds_final/test_data.txt', index=False, header=False, sep=' ')\n",
    "\n",
    "# 查看划分后的数据集大小\n",
    "print(f\"训练集大小: {len(train_data)}\")\n",
    "print(f\"测试集大小: {len(test_data)}\")\n",
    "\n",
    "# 检查是否有重复项\n",
    "print(f\"训练数据重复行数: {train_data.duplicated().sum()}\")\n",
    "print(f\"测试数据重复行数: {test_data.duplicated().sum()}\")\n",
    "duplicates_in_train = train_data.merge(test_data, how='inner')\n",
    "print(f'训练集和测试集中的重复行:\\n{duplicates_in_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
